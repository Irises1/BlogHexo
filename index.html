<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-sam related note" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/06/15/sam%20related%20note/" class="article-date">
  <time class="dt-published" datetime="2023-06-15T12:12:08.593Z" itemprop="datePublished">2023-06-15</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="sam-related-note"><a href="#sam-related-note" class="headerlink" title="sam related note"></a>sam related note</h1><h2 id="Grounded-SAM"><a href="#Grounded-SAM" class="headerlink" title="Grounded-SAM"></a><a target="_blank" rel="noopener" href="https://github.com/IDEA-Research/Grounded-Segment-Anything">Grounded-SAM</a></h2><p>背景工作</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/facebookresearch/segment-anything">Segment Anything</a>是一个强大的细分模型。但它需要提示（如框&#x2F;点）来生成掩码。</li>
<li><a target="_blank" rel="noopener" href="https://github.com/IDEA-Research/GroundingDINO">Grounding DINO</a>是一种强大的zero-shot检测器，能够生成带有自由格式文本的高质量框和标签。</li>
<li><a target="_blank" rel="noopener" href="https://osx-ubody.github.io/">OSX</a>是一种强大而高效的单阶段运动捕捉方法，可从单眼图像生成高质量的 3D 人体网格。我们还发布了一个大规模的上半身数据集 UBody，以便在上半身场景中进行更准确的重建。</li>
<li><a target="_blank" rel="noopener" href="https://github.com/CompVis/stable-diffusion">Stable-Diffusion</a>是一个惊人的强大的文本到图像扩散模型。</li>
<li><a target="_blank" rel="noopener" href="https://github.com/salesforce/lavis">BLIP</a>是用于图像理解的出色语言视觉模型。</li>
<li><a target="_blank" rel="noopener" href="https://github.com/microsoft/visual-chatgpt">Visual ChatGPT</a>是一个很棒的工具，它连接 ChatGPT 和一系列 Visual Foundation Models 以实现在聊天过程中发送和接收图像。</li>
</ul>
<p>功能：</p>
<ul>
<li><p>根据声音编辑图像</p>
<p>设计技术：<strong>Whisper + ChatGPT + Grounded-SAM + SD</strong></p>
<p><img src="https://github.com/IDEA-Research/Grounded-Segment-Anything/raw/main/assets/acoustics/gsam_whisper_inpainting_demo.png" alt="img"></p>
</li>
<li><p>半自动的标注系统</p>
<p><img src="https://github.com/IDEA-Research/Grounded-Segment-Anything/raw/main/assets/grounded_sam2.png" alt="img"></p>
<p>文本提示-&gt;目标检测-&gt;根据检测目标进行分割</p>
</li>
<li><p>数据工厂，新数据生成</p>
<p><img src="https://github.com/IDEA-Research/Grounded-Segment-Anything/raw/main/assets/grounded_sam_inpainting_demo.png" alt="img"></p>
</li>
<li><p>自动标注系统</p>
<p><img src="https://github.com/IDEA-Research/Grounded-Segment-Anything/raw/main/assets/automatic_label_output_demo3.jpg" alt="img"></p>
<p>使用 BLIP 生成caption，使用 ChatGPT 提取tags，使用 Grounded-SAM 生成框和掩码</p>
</li>
<li><p>即时3D人体网络恢复</p>
<p><img src="https://github.com/IDEA-Research/Grounded-Segment-Anything/raw/main/assets/osx/grouned_sam_osx_demo.gif" alt="img"></p>
<p>grounded-SAM生成框和mask，使用OSX估计SMPLX参数并重建3D全身</p>
</li>
<li><p>交互式编辑</p>
<p><img src="https://github.com/IDEA-Research/Grounded-Segment-Anything/raw/humanFace/assets/interactive-fashion-edit.png" alt="img"></p>
<p><img src="https://github.com/IDEA-Research/Grounded-Segment-Anything/raw/humanFace/assets/interactive-mark.gif" alt="img"></p>
<p><img src="https://github.com/IDEA-Research/Grounded-Segment-Anything/raw/humanFace/assets/231-hair-edit.png" alt="img"></p>
</li>
</ul>
<h2 id="Semantic-Segment-Anything"><a href="#Semantic-Segment-Anything" class="headerlink" title="Semantic Segment Anything"></a><a target="_blank" rel="noopener" href="https://github.com/fudan-zvg/Semantic-Segment-Anything">Semantic Segment Anything</a></h2><p>出发：SAM 缺乏为每个掩码预测语义类别的能力，SSA给每一个mask提供类别预测或者注释</p>
<p><img src="https://github.com/fudan-zvg/Semantic-Segment-Anything/raw/main/figures/sa_230745_class_name.png" alt="img"></p>
<h2 id="Segment-Anything-Model-SAM-in-Napari"><a href="#Segment-Anything-Model-SAM-in-Napari" class="headerlink" title="Segment Anything Model (SAM) in Napari"></a><a target="_blank" rel="noopener" href="https://github.com/MIC-DKFZ/napari-sam">Segment Anything Model (SAM) in Napari</a></h2><p>在Napari（一种多维图像查看器）上继承SAM来分割</p>
<p>集成还支持2D和<strong>3D</strong>图像</p>
<table>
<thead>
<tr>
<th>一切模式</th>
<th>基于点击的语义分割模式</th>
<th>基于点击的实例分割模式</th>
</tr>
</thead>
<tbody><tr>
<td><a target="_blank" rel="noopener" href="https://github.com/MIC-DKFZ/napari-sam/blob/main/cats_everything.png"><img src="https://github.com/MIC-DKFZ/napari-sam/raw/main/cats_everything.png" alt="img"></a></td>
<td><a target="_blank" rel="noopener" href="https://github.com/MIC-DKFZ/napari-sam/blob/main/cats_semantic.png"><img src="https://github.com/MIC-DKFZ/napari-sam/raw/main/cats_semantic.png" alt="img"></a></td>
<td><a target="_blank" rel="noopener" href="https://github.com/MIC-DKFZ/napari-sam/blob/main/cats_instance.png"><img src="https://github.com/MIC-DKFZ/napari-sam/raw/main/cats_instance.png" alt="img"></a></td>
</tr>
</tbody></table>
<h2 id="Open-Vocabulary-Semantic-Segmentation-with-Mask-adapted-CLIP"><a href="#Open-Vocabulary-Semantic-Segmentation-with-Mask-adapted-CLIP" class="headerlink" title="Open-Vocabulary Semantic Segmentation with Mask-adapted CLIP"></a><a target="_blank" rel="noopener" href="https://jeff-liangf.github.io/projects/ovseg/">Open-Vocabulary Semantic Segmentation with Mask-adapted CLIP</a></h2><p>CLIP结合分割模型</p>
<p>任务：开放词汇语义分割</p>
<p><img src="C:\Users\Bubble\AppData\Roaming\Typora\typora-user-images\image-20230417143510711.png" alt="image-20230417143510711"></p>
<h2 id="UniverSeg-Universal-Medical-Image-Segmentation"><a href="#UniverSeg-Universal-Medical-Image-Segmentation" class="headerlink" title="UniverSeg: Universal Medical Image Segmentation"></a><a target="_blank" rel="noopener" href="https://github.com/JJGO/UniverSeg">UniverSeg: Universal Medical Image Segmentation</a></h2><p>一种无需额外训练即可解决未见过的医学分割任务的方法。</p>
<p><img src="https://raw.githubusercontent.com/JJGO/UniverSeg/gh-pages/assets/images/network-architecture.png" alt="network"></p>
<p><img src="C:\Users\Bubble\AppData\Roaming\Typora\typora-user-images\image-20230418150557701.png" alt="image-20230418150557701"></p>
<ul>
<li><p>使用新的<strong>CrossBlock机制</strong>来生成准确的分割图，而不需要额外的训练。</p>
</li>
<li><p>收集并标准化了53个开放访问的医学分割数据集，这些数据集具有超过22000次扫描，我们称之为MegaMedical。</p>
</li>
</ul>
<p><img src="C:\Users\Bubble\AppData\Roaming\Typora\typora-user-images\image-20230418152645413.png" alt="image-20230418152645413"></p>
<h2 id="Segment-Anything-Is-Not-Always-Perfect-An-Investigation-of-SAM-on-Different-Real-world-Applications"><a href="#Segment-Anything-Is-Not-Always-Perfect-An-Investigation-of-SAM-on-Different-Real-world-Applications" class="headerlink" title="Segment Anything Is Not Always Perfect: An Investigation of SAM on Different Real-world Applications"></a>Segment Anything Is Not Always Perfect: An Investigation of SAM on Different Real-world Applications</h2><p>讨论SAM的局限性</p>
<p><img src="C:\Users\Bubble\AppData\Roaming\Typora\typora-user-images\image-20230417153952916.png" alt="image-20230417153952916"></p>
<h2 id="SAM-MD-Zero-shot-medical-image-segmentation-capabilities-of-the-Segment-Anything-Model"><a href="#SAM-MD-Zero-shot-medical-image-segmentation-capabilities-of-the-Segment-Anything-Model" class="headerlink" title="SAM.MD: Zero-shot medical image segmentation capabilities of the Segment Anything Model"></a>SAM.MD: Zero-shot medical image segmentation capabilities of the Segment Anything Model</h2><p>通过框和点的提示，将SAM推广到CT数据中</p>
<h2 id="DINOv2"><a href="#DINOv2" class="headerlink" title="DINOv2"></a><a target="_blank" rel="noopener" href="https://dinov2.metademolab.com/">DINOv2</a></h2><p>metaAI开源</p>
<p>在无监督的情况下学习视觉特征</p>
<p>网站demo三个功能</p>
<ul>
<li><p>深度估计</p>
<p><img src="C:\Users\Bubble\AppData\Roaming\Typora\typora-user-images\image-20230419095604575.png" alt="image-20230419095604575"></p>
</li>
<li><p>语义分割</p>
<p><img src="C:\Users\Bubble\AppData\Roaming\Typora\typora-user-images\image-20230419095622838.png" alt="image-20230419095622838"></p>
</li>
<li><p>实例检索</p>
<p><img src="C:\Users\Bubble\AppData\Roaming\Typora\typora-user-images\image-20230419095657723.png" alt="image-20230419095657723"></p>
</li>
<li><p>数据：提出了一种自动管道，来建立一个专门的，多样化的，精选的数据集</p>
</li>
<li><p>模型：使用1B参数训练了一个ViT后提取为一系列较小的模型</p>
</li>
</ul>
<p>自监督学习大多在一个小的策划数据集上进行预训练取得，dinov2探索在大量精选数据上进行预训练，自监督学习是否有潜力学习到目标视觉特征。</p>
<p>预训练数据集管道，灵感来自于NLP的《Ccnet: Extracting high quality monolingual datasets from web crawl data.》，使用数据相似性而不是外部元数据，且不需要手动注释。</p>
<p><img src="C:\Users\Bubble\AppData\Roaming\Typora\typora-user-images\image-20230419102417296.png" alt="image-20230419102417296"></p>
<p>对于<strong>重新平衡（rebalance）概念</strong>是难点，为了避免过度拟合一些主要模式，使用一种简单的集群模式来解决</p>
<h3 id="判别式的自监督方法"><a href="#判别式的自监督方法" class="headerlink" title="判别式的自监督方法"></a>判别式的自监督方法</h3><p>DINO and iBOT losses with the centering of SwAV （Unsupervised learning of visual features by contrasting cluster assignments.）</p>
<ul>
<li>图像级对象：从学生和教师网络中提取的特征之间的交叉熵损失</li>
<li>patch级对象：随机屏蔽学生的一些patch，不屏蔽老师，在屏蔽patch的两个网络patch特征之间添加交叉熵损失</li>
<li>Untying head weights between both objectives：两个目标相关的权重在一起会让模型patch级别上不足，在图像级上过拟合，解开这些权重会解决问题</li>
<li>Sinkhorn-Knopp centering ：使用sinkhorn knopp批量标准化替代DINO和iBot的教师softmax居中步骤</li>
<li>KoLeo regularizer</li>
<li>调整分辨率：在预训练结束时的时间内将图像的分辨率提高到518*518</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/06/15/sam%20related%20note/" data-id="clix3qjvr000fqwued6do449y" data-title="" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-Sam on tooth" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/06/15/Sam%20on%20tooth/" class="article-date">
  <time class="dt-published" datetime="2023-06-15T12:12:08.591Z" itemprop="datePublished">2023-06-15</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="Sam-on-tooth"><a href="#Sam-on-tooth" class="headerlink" title="Sam on tooth"></a>Sam on tooth</h1><p>原图</p>
<p>![2650_0000](C:\Users\Bubble\Desktop\note\medical_img_related\sam on tooth\2650_0000.png)</p>
<p>使用vit-b</p>
<p>使用默认的mask生成器结果：</p>
<p>![test](C:\Users\Bubble\Desktop\note\medical_img_related\sam on tooth\test.png)</p>
<p>vit-h模型</p>
<p><img src="C:\Users\Bubble\AppData\Roaming\Typora\typora-user-images\image-20230508162931816.png" alt="image-20230508162931816"></p>
<h1 id="异位萌出"><a href="#异位萌出" class="headerlink" title="异位萌出"></a>异位萌出</h1><p>原图</p>
<p><img src="C:\Users\Bubble\AppData\Roaming\Typora\typora-user-images\image-20230529153305711.png" alt="image-20230529153305711"></p>
<p>全牙列分割</p>
<p><img src="C:\Users\Bubble\AppData\Roaming\Typora\typora-user-images\image-20230529153646353.png" alt="image-20230529153646353"></p>
<p>异位萌出label</p>
<p><img src="C:\Users\Bubble\AppData\Roaming\Typora\typora-user-images\image-20230529153332496.png" alt="image-20230529153332496"></p>
<p>on sam：</p>
<p>默认generator</p>
<p>vit_b</p>
<p><img src="C:\Users\Bubble\AppData\Roaming\Typora\typora-user-images\image-20230529161548019.png" alt="image-20230529161548019"></p>
<p>vit_h</p>
<p><img src="C:\Users\Bubble\AppData\Roaming\Typora\typora-user-images\image-20230529155606460.png" alt="image-20230529155606460"></p>
<p>可以看到除了各个牙齿的分割，还有很大的一块整体分割的mask</p>
<p>手动剔除过大的label：</p>
<p><img src="C:\Users\Bubble\AppData\Roaming\Typora\typora-user-images\image-20230529163823743.png" alt="image-20230529163823743"></p>
<p>同样道理应该可以添加一个约束去除掉牙齿范围外的错误label</p>
<p>探索一下两个重要的参数</p>
<ul>
<li>pred_iou_thresh：使用模型预测mask的过滤阈值，用于过滤生成的没有到阈值的mask，默认值0.88</li>
<li>stability_score_thresh：[0,1]之间的过滤阈值，用于mask变化下的稳定性，默认值0.95</li>
</ul>
<p>先探索过滤阈值的变化，我觉得上面生成的大的mask是因为阈值太小了没有过滤掉这一层mask</p>
<p>将pred_iou_thresh设置为0.95（先设置大一点看看变化后微调</p>
<p>牙齿label会变少，因为产生mask的条件变严格了</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/06/15/Sam%20on%20tooth/" data-id="clix3qjvn0009qwueaj9qdq02" data-title="" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-nnUNetv2笔记" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/06/15/nnUNetv2%E7%AC%94%E8%AE%B0/" class="article-date">
  <time class="dt-published" datetime="2023-06-15T12:12:08.566Z" itemprop="datePublished">2023-06-15</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="nnUNetv2笔记"><a href="#nnUNetv2笔记" class="headerlink" title="nnUNetv2笔记"></a>nnUNetv2笔记</h1><p>分割性能保持不变，但是更方便将其用作开发框架，手动微调配置，适应新的数据集</p>
<h2 id="分层标签"><a href="#分层标签" class="headerlink" title="分层标签"></a>分层标签</h2><h3 id="是什么"><a href="#是什么" class="headerlink" title="是什么"></a>是什么</h3><p>nnUNet给了一个脑肿瘤的例子（<a target="_blank" rel="noopener" href="http://braintumorsegmentation.org/">Brain Tumor Segmentation Challenge</a>）：</p>
<p><img src="C:\Users\Bubble\AppData\Roaming\Typora\typora-user-images\image-20230321101149147.png" alt="image-20230321101149147"></p>
<p>上面的这一类label被称为individual labels，分别标记肿瘤的水肿，坏死和非增强肿瘤，增强肿瘤（edema, necrosis and non-enhancing tumor, enhancing tumor）。但是有些临床应用要检测整个肿瘤，肿瘤核心和增强肿瘤（whole tumor, tumor core and enhancing tumor）,如下图标签。</p>
<p>因此nnUNet提供了新的Region-based segmentation target，基于区域的目标分割。</p>
<h3 id="nnUNet能做什么"><a href="#nnUNet能做什么" class="headerlink" title="nnUNet能做什么"></a>nnUNet能做什么</h3><p>在训练期间，nnUNet允许通过合并单个标签构建区域来学习。当一个器官和子结构需要分割时候，可以将第一个目标区域输出为整个器官，子结构这样有层次的label。</p>
<p>注：nnUNet依然需要整数标签图（integer label maps）作为输入和提供整数标签图作为输出。基于区域目标的训练可以学习重叠标签，但是必须要对这些重叠进行建模。</p>
<h3 id="如何使用"><a href="#如何使用" class="headerlink" title="如何使用"></a>如何使用</h3><p>首先在<code>dataset.json</code>声明标签，以BraTS（上面举例的脑肿瘤分割）为例，labels定义：(这里label的整数和挑战给的整数不一样，因为nnUNet需要连续递增的整数作为label)</p>
<p>基于单个标签的写法（上图）：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="attr">&quot;labels&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;background&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;edema&quot;</span><span class="punctuation">:</span> <span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;non_enhancing_and_necrosis&quot;</span><span class="punctuation">:</span> <span class="number">2</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;enhancing_tumor&quot;</span><span class="punctuation">:</span> <span class="number">3</span></span><br><span class="line"><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>基于区域的写法（下图）：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="attr">&quot;labels&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;background&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;whole_tumor&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="number">1</span><span class="punctuation">,</span> <span class="number">2</span><span class="punctuation">,</span> <span class="number">3</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;tumor_core&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="number">2</span><span class="punctuation">,</span> <span class="number">3</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;enhancing_tumor&quot;</span><span class="punctuation">:</span> <span class="number">3</span>  # or <span class="punctuation">[</span><span class="number">3</span><span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line"><span class="attr">&quot;regions_class_order&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="number">1</span><span class="punctuation">,</span> <span class="number">2</span><span class="punctuation">,</span> <span class="number">3</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p><code>“labels”</code>给了不同区域对应实际上label的值，下面<code>&quot;regions_class_order&quot;</code>告诉nnUNet要把区域转化回整数映射。也就是将定义的区域”whole_tumor”生成label值为1，”tumor_core”生成label值为2，”enhancing_tumor”生成label值为3。每一步之后设置的label值都会去覆盖之前区域的值，因此设置”regions_class_order”时要从大范围到小范围去写。</p>
<p>注：因为区域声明对顺序很敏感，所以自动生成json时候，要确保字典的键不按照字母顺序排序，在<code>json.dump()</code>中设置 <code>sort_keys=False</code></p>
<h2 id="跨平台支持"><a href="#跨平台支持" class="headerlink" title="跨平台支持"></a>跨平台支持</h2><p>Cuda，mps（Apple M1&#x2F;M2），还有CPU支持。只需要在 <code>nnUNetv2_train</code> 使用<code>-device</code> 选择，和 <code>nnUNetv2_predict</code>中选择。</p>
<h2 id="统一训练器类"><a href="#统一训练器类" class="headerlink" title="统一训练器类"></a>统一训练器类</h2><p>定义了统一训练器类：nnUNetTrainer。所有的训练器默认功能都集中在这里。之后不要乱用cascaded trainer, DDP trainer, region-based trainer, ignore trainer等</p>
<h2 id="更多数据格式"><a href="#更多数据格式" class="headerlink" title="更多数据格式"></a>更多数据格式</h2><p>通过ImageIO类可以支持更多的输入输出格式</p>
<h2 id="不再需要nnUNet-raw-cropped-文件夹"><a href="#不再需要nnUNet-raw-cropped-文件夹" class="headerlink" title="不再需要nnUNet_raw_cropped 文件夹"></a>不再需要nnUNet_raw_cropped 文件夹</h2><p>因为裁剪保存npz文件实际上很慢，nnUNetv2改为了实时裁剪。更快的基础上节省磁盘空间</p>
<h2 id="对多GPU训练提供本地支持"><a href="#对多GPU训练提供本地支持" class="headerlink" title="对多GPU训练提供本地支持"></a>对多GPU训练提供本地支持</h2><h2 id="可以通过API访问nnUNet的所有功能"><a href="#可以通过API访问nnUNet的所有功能" class="headerlink" title="可以通过API访问nnUNet的所有功能"></a>可以通过API访问nnUNet的所有功能</h2><p><code>setup.py</code>中展示了调用的函数入口</p>
<h2 id="数据集指纹已明确创建并保存在json文件中"><a href="#数据集指纹已明确创建并保存在json文件中" class="headerlink" title="数据集指纹已明确创建并保存在json文件中"></a>数据集指纹已明确创建并保存在json文件中</h2><h2 id="plan文件检查"><a href="#plan文件检查" class="headerlink" title="plan文件检查"></a>plan文件检查</h2><p>plans.json中可以配置全局和本地设置，配置的例子和结构详情查看<a target="_blank" rel="noopener" href="https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/explanation_plans_files.md">文档</a></p>
<h2 id="文件夹结构不同"><a href="#文件夹结构不同" class="headerlink" title="文件夹结构不同"></a>文件夹结构不同</h2><p>但是更对用户友好：</p>
<ul>
<li>nnUNet_preprocessed<ul>
<li>默认情况下，预处理数据现在保存为：<code>nnUNet_preprocessed/DATASET_NAME/PLANS_IDENTIFIER_CONFIGURATION</code>明确地将它们链接到相应的计划和配置</li>
<li>包含预处理图像的文件夹的名称可以使用密钥进行调整<code>data_identifier</code>。</li>
</ul>
</li>
<li>nnUNet_results<ul>
<li>结果现在排序如下：DATASET_NAME&#x2F;TRAINERCLASS__ PLANSIDENTIFIER __CONFIGURATION&#x2F;FOLD</li>
</ul>
</li>
</ul>
<h2 id="后续未实施更新"><a href="#后续未实施更新" class="headerlink" title="后续未实施更新"></a>后续未实施更新</h2><ul>
<li>融入MONAI（官网描述：一组开源、可免费使用的协作框架，旨在加速医学成像领域的研究和临床协作。 目标是通过构建一个强大的软件框架来加快创新和临床转化的步伐，该框架几乎可以使医学成像、深度学习研究和部署的各个层面受益。）</li>
<li>大量数据集的预训练权重</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/06/15/nnUNetv2%E7%AC%94%E8%AE%B0/" data-id="clix3qjvp000cqwue1rtu2nhq" data-title="" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-MedNeXt Transformer-driven Scaling of ConvNets for Medical Image Segmentation" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/06/15/MedNeXt%20Transformer-driven%20Scaling%20of%20ConvNets%20for%20Medical%20Image%20Segmentation/" class="article-date">
  <time class="dt-published" datetime="2023-06-15T12:12:08.558Z" itemprop="datePublished">2023-06-15</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="MedNeXt-Transformer-driven-Scaling-of-ConvNets-for-Medical-Image-Segmentation"><a href="#MedNeXt-Transformer-driven-Scaling-of-ConvNets-for-Medical-Image-Segmentation" class="headerlink" title="MedNeXt: Transformer-driven Scaling of ConvNets for Medical Image Segmentation"></a>MedNeXt: Transformer-driven Scaling of ConvNets for Medical Image Segmentation</h1><p>(用于医学图像分割的Transformer驱动缩放卷积网络)</p>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>ConvNeXt试图通过镜像Transformer块来实现标准ConvNet的现代化（modernize）。基于ConvNeXt工作，设计一种现代化可扩展卷积架构来应对数据稀缺（datascarce）医疗环境的挑战。</p>
<p>MedNeXt：受Transformer启发的大内核分割网络</p>
<ol>
<li>用于医学影像分割的完全ConvNeXt 3D编码器-解码器网络</li>
<li>残差ConvNeXt上采样块和下采样块来保持跨尺度的语义丰富性</li>
<li>一种通过上采样小内核网络来迭代增加内核大小的新技术，来防止有效医疗数据带来的性能饱和</li>
<li>多个级别（深度，宽度，内核大小）复合缩放，导致CT和MRI模态和不同数据集上四种任务都能取得领先性能。</li>
</ol>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Transform作为单一架构或者混合架构在医学影像被广泛采用。</p>
<ul>
<li><p>优势：学习远程空间依赖性的能力。</p>
</li>
<li><p>缺陷：有限的归纳偏差（inductive bias）受到大型数据集标注的阻碍，不能最大限度提高性能收益。</p>
</li>
</ul>
<p>为了在使用Transformer架构同时保留卷积的归纳偏差，引入了ConvNeXt。ConvNeXt使用反瓶颈镜像Transformer，由深度层，扩展层和收缩层组成。另外还有大型深度内核来复制远距离的表示学习。但是医学影像分割中，堆叠小内核的VGGNet方法仍然是设计卷积网络的主要技术。开箱即用的数据高效解决方案，如nnUNet，标准的UNet变体在大部分任务中仍然有效。</p>
<p>ConvNeXt结合了Transformer的远程空间表示学习能力和ConvNets固有的归纳偏置。反瓶颈设计还允许在不受内核大小的影响下拓展通道。在医学影像中使用有以下好处：</p>
<ol>
<li>通过<strong>大内核</strong>学习长距离空间依赖</li>
<li>同时拓展多个网络级别</li>
</ol>
<p>为了实现这些，需要技术对抗大型网络在有限的数据上过度拟合的趋势。需要进一步探索内核的缩放问题，同时使用恒定数量的层数和通道数。ConvNeXt架构本身被用在3D-UX-Net，其中用ConvNeXt块替代了Transformer块和SwinUNETR。但是3D-UX-Net中尽在标准卷积编码器中使用限制了带来的好处。</p>
<p>Contributions：</p>
<ul>
<li>使用了一种存粹由<strong>ConvNeXt块</strong>组成的架构，使ConvNeXt块的设计在全网络范围内有优势，(2.1节)</li>
<li>引入了<strong>反残差瓶颈替代常规上采样块和下采样块</strong>，来在重新采样的同时保证上下文丰富性，有利于密集的分割任务，尤其改善了训练期间的梯度流动，(2.2节)</li>
<li>引入一种简单有效的技术，即<strong>迭代增加内核大小</strong>，UpKern，通过使用训练的上采样小内核网络进行初始化来防止大内核MedNeXts上性能饱和，(2.3节)</li>
<li>应用多个网络参数的<strong>复合缩放</strong>，允许宽度（通道数），感受野（核大小）和深度（层数）缩放的正交性，(2.4节)</li>
</ul>
<h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><h3 id="Fully-ConvNeXt-3D-Segmentation-Architecture（全ConvNeXt-3D分割架构）"><a href="#Fully-ConvNeXt-3D-Segmentation-Architecture（全ConvNeXt-3D分割架构）" class="headerlink" title="Fully ConvNeXt 3D Segmentation Architecture（全ConvNeXt 3D分割架构）"></a>Fully ConvNeXt 3D Segmentation Architecture（全ConvNeXt 3D分割架构）</h3><p>ConvNeXt块继承了Transformer很多重要的设计选择，旨在限制计算成本同时增加感受野宽度以学习全局特征。在本工作中利用这些优势采用ConvNeXt的通用设计作为3D UNet中的宏架构下的构建块来搭建MedNeXt。并且类似Transformer有三层结构。</p>
<p><img src="C:\Users\Bubble\AppData\Roaming\Typora\typora-user-images\image-20230404133954142.png" alt="image-20230404133954142"></p>
<p>这三层结构对于输入通道为C的输入可以描述为：</p>
<ol>
<li><p>Depthwise Convolution Layer（深度卷积层）</p>
<p>核大小为k，归一化（GroupNorm），输出通道为C</p>
</li>
<li><p>Expansion Layer（扩展层）</p>
<p>使用内核为1的卷积，输出CR通道，将通道C扩充R倍，然后使用GELU激活</p>
</li>
<li><p>Compression Layer（压缩层）</p>
<p>使用核为1的卷积将通道还原</p>
</li>
</ol>
<h3 id="Resampling-with-Residual-Inverted-Bottlenecks（带残差反向瓶颈的重采样）"><a href="#Resampling-with-Residual-Inverted-Bottlenecks（带残差反向瓶颈的重采样）" class="headerlink" title="Resampling with Residual Inverted Bottlenecks（带残差反向瓶颈的重采样）"></a>Resampling with Residual Inverted Bottlenecks（带残差反向瓶颈的重采样）</h3><p>ConvNeXt中的下采样是标准跨步卷积单独做的。这种方式没有隐含利用基于宽度或者内核的ConvNeXt缩放。</p>
<p>MedNeXt中：重（上，下）采样块也是MedNeXt组成，同时在第一深度卷积层中插入跨步卷积或者转置卷积。为了更容易实现梯度流，添加了核为1的卷积残差连接或者步长为2的转置卷积。</p>
<p>反残差最早在MobileNetv2提到：</p>
<p><img src="C:\Users\Bubble\AppData\Roaming\Typora\typora-user-images\image-20230406155916301.png" alt="image-20230406155916301"></p>
<h3 id="UpKern-Large-Kernel-Convolutions-without-Saturation（UpKern：无饱和的大核卷积）"><a href="#UpKern-Large-Kernel-Convolutions-without-Saturation（UpKern：无饱和的大核卷积）" class="headerlink" title="UpKern: Large Kernel Convolutions without Saturation（UpKern：无饱和的大核卷积）"></a>UpKern: Large Kernel Convolutions without Saturation（UpKern：无饱和的大核卷积）</h3><p>容易出现性能饱和，自然影像数据多，ConvNeXt在7*7的核上性能饱和，但是医学影像数据更少。</p>
<p>解决方案：参考Swin Transformer V2，使用较小的注意窗口训练另一个网络，初始化较大的注意窗口的网络。本文参考但是对卷积内核进行了定制。</p>
<p>对相同大小的内核复制预训练权重，对于不同大小的内核，使用预训练小内核网络三线性插值（Trilinear Interpolation）初始化，迭代增加内核大小。</p>
<p>swin transformer:</p>
<p><strong>Log-spaced continuous position bias</strong>:</p>
<p>对于下游任务如检测和分割往往要采用更大的分辨率，如果要采用更大的window size，那么就需要对relative position bias进行插值。</p>
<h3 id="Compound-Scaling-of-Depth-Width-and-Receptive-Field（深度、宽度和感受场的复合缩放）"><a href="#Compound-Scaling-of-Depth-Width-and-Receptive-Field（深度、宽度和感受场的复合缩放）" class="headerlink" title="Compound Scaling of Depth, Width and Receptive Field（深度、宽度和感受场的复合缩放）"></a>Compound Scaling of Depth, Width and Receptive Field（深度、宽度和感受场的复合缩放）</h3><p>在多个角度（深度，宽度，感受野，分辨率）上同时缩放带来的好处超过单一缩放带来的好处。对此设计了四种型号配置（不同参数）来实验实现。</p>
<p><img src="C:\Users\Bubble\AppData\Roaming\Typora\typora-user-images\image-20230404144428076.png" alt="image-20230404144428076"></p>
<h3 id="Experimental-Design"><a href="#Experimental-Design" class="headerlink" title="Experimental Design"></a>Experimental Design</h3><h2 id="Result-and-Discussion"><a href="#Result-and-Discussion" class="headerlink" title="Result and Discussion"></a>Result and Discussion</h2><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>因为医学影像本身的挑战（比如数据量），医学影像分割缺少从缩放网络中收益的架构。MedNeXt是一种受到Transformer启发的可扩展全ConvNeXt 3D分割架构。与用于自然影像的ConvNeXt类似，提供了复合可扩展的MedNeXt设计。</p>
<h1 id="归纳偏置（inductive-bias）"><a href="#归纳偏置（inductive-bias）" class="headerlink" title="归纳偏置（inductive bias）"></a>归纳偏置（inductive bias）</h1><p><strong>归纳偏置</strong>（英语：Inductive bias），指的是学习算法中，当学习器去预测其未遇到过的输入结果时，所做的一些<strong>假设</strong>的集合</p>
<p>机器学习任务中：</p>
<p>我们处理一些观察子集（样本），我们的目标是基于它们创建泛化。我们还希望我们的概括对新的看不见的数据有效。换句话说，我们想根据有限的样本子集得出一个适用于整个样本群的通用规则。</p>
<p>对于有限的样本集，存在无限的假设集。</p>
<p><img src="https://miro.medium.com/v2/resize:fit:574/1*RdZGjeXxq40qGaBK5LBADw.png" alt="img"></p>
<p>现在让我们从新的未见过的数据样本 X2 中推断我们的假设，事实证明大多数复杂的函数都是不准确的。但是，线性函数似乎非常准确。</p>
<p><img src="https://miro.medium.com/v2/resize:fit:565/1*pZyhF39vG5NFGL_QM7QWxQ.png" alt="img"></p>
<p>某些假设的优先级排序（假设空间的限制）是一种归纳偏差. 因此该模型偏向于某些假设组。对于前面的示例，可以根据一些关于数据的先验知识选择线性模型，从而优先考虑线性泛化。</p>
<p>选择正确的模型归纳偏差会导致更好的泛化，尤其是在低数据设置中。我们拥有的训练数据越少，归纳偏差就越强，以帮助模型更好地泛化。但在丰富的数据设置中，最好避免任何归纳偏差，让模型受到更少的约束，并自由地搜索假设空间。</p>
<p><strong>CNN</strong>：</p>
<p>CNN中的归纳偏置来源于数据和训练过程。大多数一般的 CNN 归纳偏差是局部和权重共享的。局部性意味着紧密放置的像素彼此相关。权重共享意味着搜索特定模式。图像的不同部分应该以相同的方式处理。通常在 CNN 中实施另外两个归纳偏差：具有池化层的平移不变性和不使用它们的平移等变性。</p>
<p>《Cognitive Psychology for Deep Neural Networks: A Shape Bias Case Study》对纹理和形状的三元组进行实验，发现CNN模型有纹理形状偏差，意味着更多依赖对象形状纹理而不是颜色。《Assessing Shape Bias Property of Convolutional Neural Networks》一文又有人认为CNN设计没有形状纹理偏差。</p>
<p><img src="https://miro.medium.com/v2/resize:fit:664/1*8PexH4UjYW5GFQeCRRbNpA.png" alt="img"></p>
<p>事实上，这种偏差不是由模型架构引起的，不通的数据增强也会对不同的偏差起到效果。使用冲突的形状纹理数据集还能提高模型准确性和模型鲁棒性。</p>
<p><img src="https://miro.medium.com/v2/resize:fit:700/1*I3keR_Ay0unnekawii5MQw.png" alt="img"></p>
<p><strong>Transformer</strong></p>
<p>Transformer没有很强的归纳偏置，所以更灵活，需要更多数据的模型。没有强偏差不会对模型施加额外的限制。因此，如果提供足够的数据，它可以找到更好的优化。缺点是这样的模型在低数据设置下表现更差。即使对于变压器，注入一些偏置也可能有利可图。</p>
<p>例如，在ConViT中（文章的思路就是在Transformer中引入CNN中的硬归纳偏置，通过门控限制），作者提出使用“软”卷积归纳偏置，以便他们的模型可以在必要时学会忽略它。一个模型可以从低数据设置中的卷积归纳偏差中获益，并且如果它施加了太多约束则能够忽略它。</p>
<p><img src="https://miro.medium.com/v2/resize:fit:659/1*5gxs8l2GG3wR4UV_fOf1Pw.png" alt="img"></p>
<p><img src="https://miro.medium.com/v2/resize:fit:172/1*-JlGdWZQ1Y4qeUMAWdn-GA.png" alt="img"></p>
<p><img src="https://miro.medium.com/v2/resize:fit:313/1*SSdJYIx4W5hbdoO-bJt-fw.png" alt="img"></p>
<p>对于医学影像中（因为医学影像数据不足，必须依靠归纳偏置）归纳偏置的选择：</p>
<ol>
<li>在Transformer中加入归纳偏置，（结合CNN的一些结构，或者是知识蒸馏等）</li>
<li>在CNN的基础上保留归纳偏置同时引入Transformer的一些优点。（MedNeXt和ConvNeXt就是采用了这种，文中写道：ConvNeXt架构将Vision和Swin Transformer的远程空间表示学习能力与ConvNets的固有归纳偏差相结合。）</li>
</ol>
<h1 id="A-ConvNet-for-the-2020s"><a href="#A-ConvNet-for-the-2020s" class="headerlink" title="A ConvNet for the 2020s"></a>A ConvNet for the 2020s</h1><p>主旨：使用Transformer的思维魔改ConvNet（ResNet）</p>
<p>选择的路线和涨点的路线图：</p>
<p><img src="C:\Users\Bubble\AppData\Roaming\Typora\typora-user-images\image-20230330140729726.png" alt="image-20230330140729726"></p>
<h2 id="改进的方法"><a href="#改进的方法" class="headerlink" title="改进的方法"></a>改进的方法</h2><h3 id="宏观设计"><a href="#宏观设计" class="headerlink" title="宏观设计"></a>宏观设计</h3><p>参考了Swin Transformer的设计，使用了多阶段实际使用不同特征图分辨率大小。还有两个有趣的设计值得考虑：</p>
<ul>
<li><p>the stage compute ratio</p>
<p><strong>stage ratio</strong></p>
<p>将ResNet的每个阶段块数从（3，4，6，3）调整为了（3，3，9，3），精度从78.8提高到了79.4</p>
</li>
<li><p>the “stem cell” structure</p>
<p><strong>stem to “Patchify”</strong></p>
<p>stem设计是处理输入图像的，常常用于将输入图像下采样到适当特征图大小。参考Swin Transformer的patch化，使用了一个4*4步长为4的卷积层替代ResNet的stem层，准确率从79.4提升到79.5.</p>
</li>
</ul>
<h3 id="ResNeXt"><a href="#ResNeXt" class="headerlink" title="ResNeXt"></a>ResNeXt</h3><p>使用ResNeXt的思想，核心是分组卷积，将卷积过滤器（filters）分成不同的组。ResNeXt的原则：使用更多的组，扩大宽度。</p>
<p>本文的方法：使用深度卷积，组的数量等于通道数量，深度卷积和1*1卷积的组合对空间和通道混合分离，这是ViT的共享特征。每个操作要么在空间上或者通道上混合信息，但不同时混合二者。</p>
<p>结果：降低了FLOPs，但是精度也降低了。</p>
<p>提升网络宽度：通道从64增加到96，提升FLOPs同时网络的性能也增加了</p>
<h3 id="反向瓶颈"><a href="#反向瓶颈" class="headerlink" title="反向瓶颈"></a>反向瓶颈</h3><p>Transformer中MLP层的隐藏层将通道扩大四倍，和卷积中的反向瓶颈设计相联系。由MobileNetV2推广，并受到广泛应用。</p>
<p>使用反向瓶颈使FLOPs减少，略微提高性能80.5到80.6，在另一个方案（ResNet200&#x2F;Swin-B）中带来了更大的收益81.9到82.6</p>
<h3 id="大内核size"><a href="#大内核size" class="headerlink" title="大内核size"></a>大内核size</h3><p>ViT最显著的方面是非局部子注意力，虽然之前卷积有尝试大内核，但是gold standard依然还是堆叠小内核(<code>3x3</code>)，这些在现代GPU上有高效的硬件实现。Swin Transformer将本地窗口重新引入自注意力块，但是窗口大小至少也是<code>7x7</code></p>
<ul>
<li>Moving up depthwise conv layer（上移深度卷积层）</li>
</ul>
<p>因为多头自注意力层（MSA）也在MLP之前，所以要上移深度卷积层</p>
<p><img src="C:\Users\Bubble\AppData\Roaming\Typora\typora-user-images\image-20230330154553104.png" alt="image-20230330154553104"></p>
<p>(a)残差卷积(b)反残差卷积(c)上移卷积层</p>
<ul>
<li>Increasing the kernel size（增加核大小）</li>
</ul>
<p>在之前的准备后使用了大卷积核，3，5，7，9，11都进行了实验，最终选择7x7到达饱和点。</p>
<h3 id="分层微观设计"><a href="#分层微观设计" class="headerlink" title="分层微观设计"></a>分层微观设计</h3><h4 id="Replacing-ReLU-with-GELU"><a href="#Replacing-ReLU-with-GELU" class="headerlink" title="Replacing ReLU with GELU"></a>Replacing ReLU with GELU</h4><p>GELU使ReLU更平滑的变体，在Transformer中应用，ConvNet也可以使用，尽管准确率没变化。</p>
<h4 id="Fewer-activation-functions"><a href="#Fewer-activation-functions" class="headerlink" title="Fewer activation functions"></a>Fewer activation functions</h4><p>Transformer和ResNet一个小区别是Transformer有较少的激活函数。Transformer只有在MLP中有一个激活函数，但是在卷积里，激活函数附加到每个卷积层，本文做了一些实验来研究性能如何变化。</p>
<p>结果选择：</p>
<p><img src="C:\Users\Bubble\AppData\Roaming\Typora\typora-user-images\image-20230330160850857.png" alt="image-20230330160850857"></p>
<h4 id="Fewer-normalization-layers"><a href="#Fewer-normalization-layers" class="headerlink" title="Fewer normalization layers"></a>Fewer normalization layers</h4><p>Transformer通常也有较少的normalization层，ConvNeXt也根据实验去掉了很多Normalization层，甚至比Transformer更少。</p>
<h4 id="Substituting-BN-with-LN"><a href="#Substituting-BN-with-LN" class="headerlink" title="Substituting BN with LN"></a>Substituting BN with LN</h4><p>BN一直是视觉任务首选，提高收敛性能并减少过拟合，但是也会对性能产生不利影响。LN更简单，Transformer中首选了LN并取得了良好的性能。原始ResNet中直接替代BN将导致性能次优，这里对网络框架和训练技术的修改，这里获得了更好的性能。</p>
<h4 id="Separate-downsampling-layers"><a href="#Separate-downsampling-layers" class="headerlink" title="Separate downsampling layers"></a>Separate downsampling layers</h4><p>Swin Transformer在stage之间添加了一个单独下采样层，ResNet的下采样是通过每个阶段开始的残差块来实现的（3*3卷积，步长2）。这里探索了一种类似Swin Transformer的策略，使用步长为2，核为2的卷积进行空间下采样，在空间分辨率改变的地方添加了归一化层还可以帮助稳定训练。显著提高准确率。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/06/15/MedNeXt%20Transformer-driven%20Scaling%20of%20ConvNets%20for%20Medical%20Image%20Segmentation/" data-id="clix3qjvj0004qwueb4y6gfjp" data-title="" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-异位萌出任务" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/06/15/%E5%BC%82%E4%BD%8D%E8%90%8C%E5%87%BA%E4%BB%BB%E5%8A%A1/" class="article-date">
  <time class="dt-published" datetime="2023-06-15T12:12:08.550Z" itemprop="datePublished">2023-06-15</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="异位萌出任务"><a href="#异位萌出任务" class="headerlink" title="异位萌出任务"></a>异位萌出任务</h1><p>病灶定位</p>
<p>对于医学影像固定组织的疾病分类，通常在分类之前需要定位到位置进行ROI提取。对于口腔第一恒磨牙异位萌出分类任务，需要先分割出全牙列，然后根据牙位提取出第一恒磨牙出的异位萌出ROI。但是对于罕见疾病直接分割效果很差，无法提取出</p>
<p>为了提取出准确的ROI，本文使用大模型SAM对图像进行初步分割，提取出影像中满足一定IoU的mask。将这些mask与原图作为不同的通道输入分割专家分割网络，分割出目标病灶以提取病灶。</p>
<h1 id="儿童牙文章"><a href="#儿童牙文章" class="headerlink" title="儿童牙文章"></a>儿童牙文章</h1><h2 id="Artificial-Intelligence-Its-Uses-and-Application-in-Pediatric-Dentistry-A-Review"><a href="#Artificial-Intelligence-Its-Uses-and-Application-in-Pediatric-Dentistry-A-Review" class="headerlink" title="Artificial Intelligence Its Uses and Application in Pediatric Dentistry: A Review"></a>Artificial Intelligence Its Uses and Application in Pediatric Dentistry: A Review</h2><p>人工智能在儿童牙应用综述</p>
<ul>
<li><p>牙菌斑</p>
</li>
<li><p>评估儿童口腔健康</p>
</li>
<li><p>中齿和多齿鉴定</p>
<p>基于CNN的深度学习模型检测初级牙列或者混合牙列的中齿牙</p>
<p>具有在全景X线片检测上颌埋伏多生牙的潜力</p>
</li>
<li><p>幼儿龋齿</p>
</li>
<li><p>裂缝封闭剂分类</p>
</li>
<li><p>使用神经网络评估儿童的实足年龄</p>
</li>
<li><p>检查乳牙及幼齿</p>
</li>
<li><p>第一恒磨牙异位出牙</p>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/06/15/%E5%BC%82%E4%BD%8D%E8%90%8C%E5%87%BA%E4%BB%BB%E5%8A%A1/" data-id="clix3qjvt000kqwuego3hhnwz" data-title="" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-口腔即刻种植项目笔记" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/06/15/%E5%8F%A3%E8%85%94%E5%8D%B3%E5%88%BB%E7%A7%8D%E6%A4%8D%E9%A1%B9%E7%9B%AE%E7%AC%94%E8%AE%B0/" class="article-date">
  <time class="dt-published" datetime="2023-06-15T12:12:08.543Z" itemprop="datePublished">2023-06-15</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="文件结构"><a href="#文件结构" class="headerlink" title="文件结构"></a>文件结构</h1><p>img&#x2F;img_id&#x2F;name-id.dcm</p>
<p>label&#x2F;img_id.nii.gz</p>
<h1 id="读取Dicom"><a href="#读取Dicom" class="headerlink" title="读取Dicom"></a>读取Dicom</h1><ol>
<li><p>路径包含中文，重命名复制存放后读取</p>
</li>
<li><p>按照slice的ImagePositionPatient排序切片，组成三维图像</p>
</li>
<li><p>归一化(读取窗宽窗位，落到0-1采样5%-95%之间)：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">imgHu = (imgHu+(ww/<span class="number">2</span>-wc))/ww</span><br><span class="line">imgHu = np.clip(imgHu, <span class="number">0.05</span>, <span class="number">0.95</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>根据label带的<code>Origin()</code>,<code>Spacing()</code>,<code>Direction()</code>属性给三维图像添加三维信息</p>
</li>
<li><p>输出存为nifit文件(.nii.gz格式)</p>
</li>
</ol>
<p>处理过程中用到的一些信息：sapcing代表一个体素的物理分辨率，单位mm，ww和wc是hu值的窗宽窗位</p>
<p><img src="C:\Users\Bubble\AppData\Roaming\Typora\typora-user-images\image-20230328153859991.png" alt="image-20230328153859991"></p>
<h1 id="实验细节"><a href="#实验细节" class="headerlink" title="实验细节"></a>实验细节</h1><p>GPU：4张RTX2080Ti</p>
<p>CPU：Intel Xeon Silver 4214</p>
<p>操作系统：Ubuntu(Linux)</p>
<p>Python：3.9</p>
<p>pytorch：1.12.1</p>
<p>epoch：200</p>
<p>框架初步使用nnUNetv2</p>
<h1 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h1><p>验证集（3例）平均：</p>
<p><img src="C:\Users\Bubble\AppData\Roaming\Typora\typora-user-images\image-20230406214858867.png" alt="image-20230406214858867"></p>
<p>训练集</p>
<p>训练结果不太理想，主要问题在于把11和12两颗牙和分别的牙槽骨识别反了，以及一些细节方面，452效果最好，451和453效果不佳</p>
<p>451：</p>
<p><img src="C:\Users\Bubble\AppData\Roaming\Typora\typora-user-images\image-20230425103131203.png" alt="image-20230425103131203"></p>
<p>452：</p>
<p><img src="C:\Users\Bubble\AppData\Roaming\Typora\typora-user-images\image-20230409135657738.png" alt="image-20230409135657738"></p>
<p>453：</p>
<p><img src="C:\Users\Bubble\AppData\Roaming\Typora\typora-user-images\image-20230425103550694.png" alt="image-20230425103550694"></p>
<h1 id="第一阶段：端到端的分割框架"><a href="#第一阶段：端到端的分割框架" class="headerlink" title="第一阶段：端到端的分割框架"></a>第一阶段：端到端的分割框架</h1><p>直接输入CBCT影像，得到牙齿和牙槽骨分割结果</p>
<p>问题：</p>
<ul>
<li>牙槽骨分割结果较差</li>
<li>全图编码导致计算开销大，效率低</li>
<li>两颗前牙容易识别反</li>
</ul>
<h1 id="第二阶段"><a href="#第二阶段" class="headerlink" title="第二阶段"></a>第二阶段</h1><p>更改任务的设计</p>
<p>问题</p>
<ol>
<li><p>是否需要提取ROI：</p>
<p>使用ROI训练不能改变目前的问题</p>
<p>但是提取可以加快训练速度</p>
</li>
<li><p>数据分布上有什么问题</p>
<p>451，452，453三例，451和453识别上容易出错，尤其是两颗牙相反的情况，多次训练不同模型取得的效果都类似</p>
<p>使用五折的另外作为训练集和验证集也依旧在某几例上识别相反</p>
</li>
<li><p>是否要分离牙齿和牙槽骨分割任务</p>
<p>实验室100例cbct只含牙齿标注，可以加入解决数据分布问题，但是没有提取ROI训练耗时很长</p>
<p>在分割牙齿后使用牙齿数据作为辅助再去分割牙槽骨，本身牙槽骨的dice也不是很理想</p>
</li>
</ol>
<p>思路总结：</p>
<ol>
<li><p>端到端直接分割出目标四个区域</p>
<p>问题：</p>
<ul>
<li>牙齿与牙槽骨之间准确率差距比较大</li>
<li>左右牙在卷积中难分辨</li>
</ul>
</li>
<li><p>设计多任务分割管道</p>
<ul>
<li>训练网络，提取roi</li>
<li>对牙齿训练分割网络</li>
<li>加上牙齿的辅助对牙槽骨训练分割网络</li>
</ul>
</li>
</ol>
<p>拓展思路</p>
<ul>
<li><p>数据引擎(难，周期长)</p>
</li>
<li><p>promopt</p>
<p>给先验知识一种其他的概念</p>
</li>
<li><p>文本+图像</p>
<p>诊断过程的文本信息</p>
</li>
<li><p>zero-shot(零样本)</p>
<p>SAM为base</p>
</li>
</ul>
<p>汇报内容：</p>
<p>目前项目进展与结果：（数据，结果）</p>
<p>第一阶段分割流程图</p>
<p>技术分享</p>
<p>后续计划</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/06/15/%E5%8F%A3%E8%85%94%E5%8D%B3%E5%88%BB%E7%A7%8D%E6%A4%8D%E9%A1%B9%E7%9B%AE%E7%AC%94%E8%AE%B0/" data-id="clix3qjvs000iqwuegs2vgi9f" data-title="" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-交接相关" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/06/15/%E4%BA%A4%E6%8E%A5%E7%9B%B8%E5%85%B3/" class="article-date">
  <time class="dt-published" datetime="2023-06-15T12:12:08.536Z" itemprop="datePublished">2023-06-15</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>数据情况</p>
<p>&#x2F;data&#x2F;yw&#x2F;tooth53是这一次项目那边医生的数据，其中<code>16</code>文件夹下是标注和原图，<code>test</code>是三例测试图像和标注，<code>img</code>是所有的图像数据</p>
<p><code>data_load.py</code>是读取dcm序列转nifiti格式数据的python脚本，但是对于没有标注的数据(img下的一些没标注过的)，需要调整一下，因为是读取原label的spacing等三个信息写入的，改成从dcm中读就行</p>
<p>&#x2F;data&#x2F;yw&#x2F;tooth100是实验室中的100例cbct数据，其中label_12是只有前面两颗牙的label(适合我们项目的任务提取的),label_all是所有牙齿的标注</p>
<p>&#x2F;data&#x2F;yw&#x2F;nnUNet_data是nnUNet使用的数据文件，如果要直接处理完成的数据可以来这里找</p>
<ul>
<li>400：端到端，直接输入图像分割四个mask</li>
<li>401：16个数据根据label取roi的训练结果</li>
<li>402：所有100+16例数据直接输入训练两颗牙，里面有两例数据有小问题我给挑出来没加入训练</li>
<li>405：使用16个数据的标注提取ROI的网络，可以直接用这个模型推理得到牙齿ROI，其下有roi.py，可以根据推理的roi区域裁剪原图和原标注文件到指定位置</li>
<li>406：提取roi之后对100+数据进行训练，提取前牙</li>
</ul>
<p>对16例数据我一般设置epoch都比较小，在200，提取roi不需要很精准所以只设置了100，对于100+数据建议设置400epoch</p>
<p>服务器：10.12.44.105:9000</p>
<p>用户名:yw，密码：qpwoei12!@</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/06/15/%E4%BA%A4%E6%8E%A5%E7%9B%B8%E5%85%B3/" data-id="clix3qjvr000gqwue8x0qdbkn" data-title="" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-Segment Anything" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/06/15/Segment%20Anything/" class="article-date">
  <time class="dt-published" datetime="2023-06-15T12:12:08.528Z" itemprop="datePublished">2023-06-15</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="Segment-Anything"><a href="#Segment-Anything" class="headerlink" title="Segment Anything"></a>Segment Anything</h1><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>大型语言模型正通过强大的零样本和少样本泛化改变NLP，模型可以泛化到超出训练期间的任务和数据分布。这些工作通过提示工程来实现，使用手动输入文本来提示语言模型给目前任务生成有效文本响应。（说明ChatGPT之类的模型的影响）</p>
<p>基础模型在视觉上的探索：CLIP和ALIGN使用对比学习来训练对齐两种模式的文本和图像编码器，还可以组合其他模块支持下游任务。但是视觉超出这些范围的广泛问题，和大多数问题没有丰富的训练数据。</p>
<p>本工作：</p>
<p>目标建立一个图像分割基础模型，寻求一个可提示的模型并能实现强大的泛化任务。在大型数据集上进行预训练，使用及时工程解决数据分布上的一系列上下游分割问题。</p>
<p>关键在三个部分：</p>
<ul>
<li><p>任务</p>
<p>What task will enable zero-shot generalization?</p>
<p>定义了一个<strong>可提示的分割任务</strong>，任务足够通用，可以提供强大的训练目标并实现广泛的下游应用</p>
</li>
<li><p>模型</p>
<p>What is the corresponding model architecture?</p>
<p>一个<strong>支持灵活提示的模型</strong>，可以在提示时实时输出mask，以便进行交互</p>
</li>
<li><p>数据</p>
<p>What data can power this task and model?</p>
<p>需要一个多样化的大规模的数据源，但是没有这种用于分割的网络规模的数据源。因此构建了一个“<strong>数据引擎</strong>”：使用高效的模型来协助数据收集和使用新收集的数据来改进模型之间进行迭代。</p>
</li>
</ul>
<p><img src="C:\Users\Bubble\AppData\Roaming\Typora\typora-user-images\image-20230410155322999.png" alt="image-20230410155322999"></p>
<h3 id="可提示的分割任务"><a href="#可提示的分割任务" class="headerlink" title="可提示的分割任务"></a>可提示的分割任务</h3><p>目标是给定任何分割提示的情况下返回有效的分割掩码。提示只指定在图像中分割什么，有效分割掩码意味着即使提示不准确或者有多个对象也应该输出一个合理掩码。使用可提示分割任务作为与训练目标并通过提示工程解决一般下游分割任务。</p>
<p><img src="C:\Users\Bubble\AppData\Roaming\Typora\typora-user-images\image-20230410145909443.png" alt="image-20230410145909443"></p>
<h3 id="模型体系结构"><a href="#模型体系结构" class="headerlink" title="模型体系结构"></a>模型体系结构</h3><p>模型必须支持灵活提示，需要实时计算掩码以允许交互式使用且必须具有模糊性。</p>
<p>一个简单的设计满足了三个约束：</p>
<ul>
<li>一个强大的图像编码器计算图像嵌入</li>
<li>一个提示编码器嵌入提示</li>
<li>将两个信息源组合在一个预测分割掩码的轻量级掩码解码器</li>
</ul>
<p>因此提出SAM模型，通过将SAM分离为图像编码器和快速提示编码器&#x2F;掩码解码器，可以在不同提示下重用相同图像嵌入。</p>
<p>提示上专注点，框和掩码提示，通过自由形式的文本提示显示初始结果。为了让SAM意识歧义，将其设计为预测单个提示的多个掩码来自然处理歧义。</p>
<p><img src="C:\Users\Bubble\AppData\Roaming\Typora\typora-user-images\image-20230410145922457.png" alt="image-20230410145922457"></p>
<h3 id="数据引擎"><a href="#数据引擎" class="headerlink" title="数据引擎"></a>数据引擎</h3><p>为了数据强泛化，需要在一组庞大而多样的掩码上训练SAM，在线获取数据的mask不自然丰富，因此使用一种数据引擎方案。</p>
<p>数据引擎：<br>模型在环数据集注释共同开发模型，共有三个阶段</p>
<ol>
<li>SAM帮助注释器注释掩码，类似于经典的交互式分段设计</li>
<li>SAM通过提示可能的对象位置来自动生成对象子集的掩码，注释器专注于注释剩余对象</li>
<li>用前景点的规则网格提示SAM，平均每张图像产生约100个高质量masks</li>
</ol>
<p><img src="C:\Users\Bubble\AppData\Roaming\Typora\typora-user-images\image-20230410145930219.png" alt="image-20230410145930219"></p>
<h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>最终数据集SA-1B包括来自11M图像，超过1B的掩码。是数据引擎最后阶段完全自动收集的，掩码数量比现有的数据集都多400倍。</p>
<h2 id="Segment-Anything-Task"><a href="#Segment-Anything-Task" class="headerlink" title="Segment Anything Task"></a>Segment Anything Task</h2><p>NLP中下一个token预测任务用于基础模型的预训练，通过即时工程解决不同下游任务，分割基础模型也需要定义一个具有类似功能的任务。</p>
<h4 id="任务"><a href="#任务" class="headerlink" title="任务"></a>任务</h4><p>首先将提示的概念从NLP转换到分割，提示可以是一组前景&#x2F;背景点，粗略框或者掩码，自由格式的文本，或者指示分割什么的任何信息。</p>
<p>可提示分割任务：在给定任何提示的情况下返回有效的分割掩码，有效的分割掩码在Intro有说明。因为它会产生一个自然的预训练算法和一个通过提示将零样本转移到下游分割任务的通用方法。</p>
<h4 id="预训练"><a href="#预训练" class="headerlink" title="预训练"></a>预训练</h4><p>一个自然的预训练算法，模拟每个训练样本的提示序列（例如：点，框，掩码），并将模型的掩码预测与基本事实进行比较。</p>
<p>交互式分割：在足够的用户输入后最终预测有效的掩码</p>
<p>本文的目的：始终预测任何提示的有效掩码，即使提示不准确，包括数据引擎需要的自动注释。需要专门建模和训练损失的选择。</p>
<h4 id="零样本迁移"><a href="#零样本迁移" class="headerlink" title="零样本迁移"></a>零样本迁移</h4><h4 id="相关任务"><a href="#相关任务" class="headerlink" title="相关任务"></a>相关任务</h4><p>分割的范围包括：交互式分割，边缘检测，超级像素化，对象建议生成，前景分割，语义分割，实例分割，全景分割等。</p>
<p>目标是使模型适应许多现有的和新的分割任务，这种能力是任务泛化的一种形式。与之前的多任务分割系统的工作不同，可提示分割训练的模型可以在推理时通过更大系统的组件来执行新的不同任务。</p>
<h2 id="Segment-Anything-Model"><a href="#Segment-Anything-Model" class="headerlink" title="Segment Anything Model"></a>Segment Anything Model</h2><p><img src="C:\Users\Bubble\AppData\Roaming\Typora\typora-user-images\image-20230410155322999.png" alt="image-20230410155322999"></p>
<p>模型如图有三个组件：图像编码器，灵活提示编码器和快速掩码解码器</p>
<h3 id="图像编码器"><a href="#图像编码器" class="headerlink" title="图像编码器"></a>图像编码器</h3><p>使用MAE的预训练ViT</p>
<h3 id="提示编码器"><a href="#提示编码器" class="headerlink" title="提示编码器"></a>提示编码器</h3><p>考虑两组提示：稀疏（点，框，文本）和密集（掩码）。使用位置编码来表示点和框，位置编码和每个提示类型的学习嵌入相加，然后使用CLIP现成文本编码器来表示自由格式文本。密集提示用卷积嵌入，并与图像嵌入逐元素求和。</p>
<h3 id="掩码解码器"><a href="#掩码解码器" class="headerlink" title="掩码解码器"></a>掩码解码器</h3><p>对解码器块修改，动态掩码预测头。在两个方向上使用提示自注意和交叉注意来更新所有嵌入。之后对图像嵌入上采样，MLP将输出token映射到动态线性分类器计算每个图像位置的掩码前景概率。</p>
<p><img src="C:\Users\Bubble\AppData\Roaming\Typora\typora-user-images\image-20230510154603923.png" alt="image-20230510154603923"></p>
<p>​    	两层解码器通过交叉关注来更新图像嵌入和prompt token。然后，图像嵌入被放大，从中更新的输出token被用来动态预测掩码。（为图清晰起见，未进行说明：在每个关注层，位置编码被添加到图像嵌入中，整个原始prompt token（包括位置编码）被重新添加到token的 query和key中</p>
<ul>
<li><p>在prompt embedding进入decoder之前，先在它上面concat了一组可学习的output tokens，output tokens由两个部分构成：</p>
</li>
<li><ul>
<li>一个是iou token，它会在后面被分离出来用于预测iou的可靠性（对应结构图右侧的IoU output token），它受到模型计算出的iou与模型计算出的mask与GT实际的iou之间的MSE loss监督；</li>
<li>另一个是mask token，它也会在后面被分离出来参与预测最终的mask（对应结构图右侧的output token per mask），mask受到focal loss和dice loss 20:1的加权组合监督。</li>
<li>这两个token的意义我感觉比较抽象，因为理论来说进入decoder的变量应该是由模型的输入，也就是prompt和image的映射构成，但这两个token的定义与prompt和image完全没有关系，而是凭空出现的。从结果反推原因，只能把它们理解成对模型的额外约束，因为它们两个参与构成了模型的两个输出并且有loss对他们进行监督。</li>
<li>最终prompt embedding（这一步改名叫prompt token）和刚才提到这两个token concat到一起统称为tokens进入decoder。</li>
</ul>
</li>
<li><p>image embedding在进入decoder之前也要进行一步操作：dense prompt由于包含密集的空间信息，与image embedding所在的特征空间一致性更高，所以直接与image embedding相加融合。因为后面要与prompt做cross attention融合，这里还要先算一下image embedding的位置编码。</p>
</li>
<li><p>接下来{image embedding，image embedding的位置编码，tokens}进入一个两层transformer结构的decoder做融合。值得注意的是，在transformer结构中，为了保持位置信息始终不丢失，每做一次attention运算，不管是self-attention还是cross-attention，tokens都叠加一次初始的tokens，image embedding都叠加一次它自己的位置编码，并且每个attention后边都接一个layer_norm。</p>
</li>
<li><ul>
<li>tokens先过一个self-attention。</li>
<li>tokens作为q，对image embedding做cross attention，更新tokens。</li>
<li>tokens再过两层的mlp做特征变换。</li>
<li>image embedding作为q，对tokens做cross attention，更新image embedding。</li>
</ul>
</li>
<li><p>更新后的tokens作为q，再对更新后的image embedding做cross attention，产生最终的tokens。</p>
</li>
<li><p>更新后的image embedding过两层kernel_size&#x3D;2, stride&#x3D;2的转置卷积，升采样到4x大小（依然是4x降采样原图的大小），产生最终的image embedding。</p>
</li>
<li><p>接下来兵分两路：</p>
</li>
<li><ul>
<li>mask token被从tokens中分离出来（因为他一开始就是concat上去的，可以直接按维度摘出来），过一个三层的mlp调整channel数与最终的image embedding一致，并且他们两个做矩阵乘法生成mask的预测。</li>
<li>iou token被从tokens中分离出来，也过一个三层的mlp生成最终的iou预测。</li>
</ul>
</li>
<li><p>最后，如前文所述，分别对mask的预测和iou预测进行监督，反向传播，更新参数。</p>
</li>
</ul>
<p>阅读参考文章</p>
<h4 id="参考1：End-to-end-object-detection-with-Transformers"><a href="#参考1：End-to-end-object-detection-with-Transformers" class="headerlink" title="参考1：End-to-end object detection with Transformers."></a><strong>参考1：End-to-end object detection with Transformers.</strong></h4><p><img src="C:\Users\Bubble\AppData\Roaming\Typora\typora-user-images\image-20230511145716266.png" alt="image-20230511145716266"></p>
<p><img src="C:\Users\Bubble\AppData\Roaming\Typora\typora-user-images\image-20230511162953470.png" alt="image-20230511162953470"></p>
<h4 id="参考2：Per-Pixel-Classification-is-Not-All-You-Need-for-Semantic-Segmentation（NeurIPS2021）"><a href="#参考2：Per-Pixel-Classification-is-Not-All-You-Need-for-Semantic-Segmentation（NeurIPS2021）" class="headerlink" title="参考2：Per-Pixel Classification is Not All You Need for Semantic Segmentation（NeurIPS2021）"></a>参考2：Per-Pixel Classification is Not All You Need for Semantic Segmentation（NeurIPS2021）</h4><p>语义分割中根据像素分类来产生语义分割，掩码分类的替换范式：将图像分割和分割的分类分开</p>
<p>提出了一种简单的MaskFormer方法，该方法可以将任何现有的每像素分类模型无缝转换为掩码分类。使用DETR中提出的集合预测机制，MaskFormer使用Transformer解码器来计算一组对，每个对由类预测和掩码嵌入向量组成。掩码嵌入向量用于通过点积获得二进制掩码预测，其中每像素嵌入从底层全卷积网络获得。新模型以统一的方式解决了语义级和实例级的分割任务：不需要更改模型、损失和训练过程。具体来说，对于语义和全景分割任务，MaskFormer是在相同的每像素二进制掩码损失和每掩码单个分类损失的情况下进行监督的。最后，我们设计了一个简单的推理策略，将MaskFormer的输出混合到任务相关的预测格式中。</p>
<p><img src="C:\Users\Bubble\AppData\Roaming\Typora\typora-user-images\image-20230510153836932.png" alt="image-20230510153836932"></p>
<p><img src="C:\Users\Bubble\AppData\Roaming\Typora\typora-user-images\image-20230511142312098.png" alt="image-20230511142312098"></p>
<p>​		transformer decoder的N个嵌入（不需要对应k个类别），计算对k类的每个概率，如果都不属于就归为背景，也可以多个嵌入对应一个类别。</p>
<h3 id="模糊解"><a href="#模糊解" class="headerlink" title="模糊解"></a>模糊解</h3><p>修改模型预测单个提示的多个输出掩码，三个掩码输出足够解决大多数情况，为了对掩码排序，模型预测每个掩码的置信度得分（IoU）</p>
<p><img src="C:\Users\Bubble\AppData\Roaming\Typora\typora-user-images\image-20230410165045721.png" alt="image-20230410165045721"></p>
<h3 id="Loss和训练"><a href="#Loss和训练" class="headerlink" title="Loss和训练"></a>Loss和训练</h3><p>loss使用focal loss和dice loss的结合，使用几何提示的混合来训练可提示的分割任务，通过在每个掩码的11轮种随机采样提示来模拟交互式设置，使SAM无缝集成到数据引擎中。</p>
<h2 id="Segment-Anything-Data-Engine"><a href="#Segment-Anything-Data-Engine" class="headerlink" title="Segment Anything Data Engine"></a>Segment Anything Data Engine</h2><p>数据引擎三个阶段</p>
<ol>
<li><p>模型辅助的手动标注阶段</p>
<p>与经典交互式分割相同，基于浏览器实时分割模型运行，不对标记对象施加语义约束，注释器也可以自由标记事物和东西（标记可以命名或者描述的对象，但是不收集这些描述）</p>
<p><strong>问题：类似ChatGPT与本文之类的模型怎么做实时交互的</strong></p>
<p>这里数据集是公共数据集，然后用新生成的掩码对SAM再训练，mask会越来越多，编码器也会变深，总共进行6次再训练，从12万张图像收集到了430万个mask</p>
</li>
<li><p>混合了自动预测掩码和模型辅助注释的半自动阶段</p>
<p>这个阶段目的是增加mask的多样性，来提高模型分割任何东西的能力。为了让注释器集中在不突出对象，先检测到置信度高的对象的mask，然后要求注释其他未注释的对象。为了检测置信度高的mask，使用通用的对象类别在第一阶段手工标注时训练了一个边界框检测器。定期对新收集的数据进行再训练。</p>
</li>
<li><p>全自动阶段，在没有注释器输入的情况下生成掩码</p>
<p>已经开发模糊感知模型，能够预测有效的mask，即使在模糊情况下。使用32*32的网格提示模型，每一个点都预测一组可能对应于有效对象的掩码。如果这个点在某个部分或者子部分上，模型会返回子部分，部分和整个对象。使用IoU预测模块选择置信掩码。只识别选择稳定的mask。然后应用非极大值抑制过滤重复。为了提高小mask的质量，处理了多个重叠放大图像的裁剪。最后生成了11亿的高质量mask。</p>
</li>
</ol>
<h2 id="Segment-Anything-Dataset"><a href="#Segment-Anything-Dataset" class="headerlink" title="Segment Anything Dataset"></a>Segment Anything Dataset</h2><p>说明和公开了分割数据集</p>
<h2 id="Zero-Shot-Transfer-Experiments"><a href="#Zero-Shot-Transfer-Experiments" class="headerlink" title="Zero-Shot Transfer Experiments"></a>Zero-Shot Transfer Experiments</h2><h3 id="Zero-Shot-Text-to-Mask"><a href="#Zero-Shot-Text-to-Mask" class="headerlink" title="Zero-Shot Text-to-Mask"></a>Zero-Shot Text-to-Mask</h3><p>从自由形式的文本中分割对象，将SAM的训练过程变为有文本意识，但是不需要新的文本注释。对于Mask，提取CLIP图像嵌入，作为SAM的第一次交互来提示SAM。CLIP图像嵌入被训练成和文本嵌入对齐，使用图像嵌入训练但是使用文本嵌入推理</p>
<h1 id="Learning-Transferable-Visual-Models-From-Natural-Language-Supervision"><a href="#Learning-Transferable-Visual-Models-From-Natural-Language-Supervision" class="headerlink" title="Learning Transferable Visual Models From Natural Language Supervision"></a>Learning Transferable Visual Models From Natural Language Supervision</h1><p>关于文本与图像嵌入的关联，segment anything频繁提到CLIP来自于本文，来自2021年的OpenAI</p>
<h2 id="Introduction-1"><a href="#Introduction-1" class="headerlink" title="Introduction"></a>Introduction</h2><p>本文的出发点是直接从网络文本中学习的可扩展预训练方法在计算机视觉上应用</p>
<p>创建了一个由4亿对（图像、文本）组成的新数据集，并证明了从头开始训练的ConVIRT的简化版本，称之为CLIP，用于对比语言图像预训练，是一种从自然语言监督中学习的有效方法。</p>
<p>发现，CLIP与GPT家族类似，在预训练期间学习执行一系列任务，包括OCR、地理定位、动作识别和许多其他任务。</p>
<p>参考下面的第三篇文章的ConVIRT工作</p>
<p>方法图</p>
<p><img src="C:\Users\Bubble\AppData\Roaming\Typora\typora-user-images\image-20230412161610900.png" alt="方法图"></p>
<h1 id="contrastive-learning-of-medical-visual-representations-from-paired-images-and-text"><a href="#contrastive-learning-of-medical-visual-representations-from-paired-images-and-text" class="headerlink" title="contrastive learning of medical visual representations from paired images and text"></a>contrastive learning of medical visual representations from paired images and text</h1><p>领域是对比学习结合多模态</p>
<ul>
<li>学习医学图像的视觉表示是医学图像的核心了解，但它的进展一直受到手工标签的小尺寸数据集的阻碍；</li>
<li>现有工作通常依赖于从 ImageNet 预训练的模型，由于图像特征完全不同，表现并不好；</li>
<li>或从与医疗配对的文本报告数据中提取基于规则的标签图像，标签不准确且难以概括；</li>
</ul>
<p><img src="C:\Users\Bubble\AppData\Roaming\Typora\typora-user-images\image-20230412151630468.png" alt="image-20230412151630468"></p>
<ul>
<li>一张图片先做随机裁剪，再接一个数据增强，然后进入Image Encoder(ResNet50),最后接一个MLP得到512维的特征表示；</li>
<li>与这张图片配对的一段话，随机采样其中一部分(几句话，或者不完整的几句)，然后进入text Encoder(Bert),最后接一个MLP得到512维的特征表示；</li>
<li>因为一个batch中有N个图片文本对，所以可以理解为有N-1个负例，只有一个正例，然后分别对图片和文本计算infoNCE loss；</li>
</ul>
<p>这里文本一般是文本报告</p>
<h2 id="Zero-shot工作"><a href="#Zero-shot工作" class="headerlink" title="Zero-shot工作"></a>Zero-shot工作</h2><p>Image-Image</p>
<p>使用查询图像搜索特定类别的图像，对查询图像和所有候选图像编码，按照相似度对候选进行排序，候选图像都包含分类标签</p>
<p>Text-Image</p>
<p>对于候选图像，8个异常类别，每一个类别写5个不同的，有代表性的文本描述，给每个查询使用文本编码器进行编码然后再候选图像中检索。这种评估图像同时评估文本表示和图像表示之间的对齐</p>
<h1 id="Code-SAM"><a href="#Code-SAM" class="headerlink" title="Code(SAM)"></a>Code(SAM)</h1><h2 id="model"><a href="#model" class="headerlink" title="model"></a>model</h2><p>一些参数</p>
<p>构建SAM模型需要定义：</p>
<ul>
<li>image encoder</li>
<li>prompt encoder</li>
<li>mask decoder</li>
<li>pixel mean</li>
<li>pixel std</li>
</ul>
<p>这里使用均值和标准差的归一化</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">prompt_embed_dim = 256</span><br><span class="line">image_size = 1024</span><br><span class="line">vit_patch_size = 16</span><br><span class="line">image_embedding_size = image_size // vit_patch_size</span><br></pre></td></tr></table></figure>

<h3 id="输入模型接收的参数"><a href="#输入模型接收的参数" class="headerlink" title="输入模型接收的参数"></a>输入模型接收的参数</h3><ul>
<li><p>batched_input：List[Dict[str, Any]]</p>
<p>输入图像的列表，字典包括以下内容，prompt key不存在可以忽略</p>
<ul>
<li>image：已经被模型转成tensor的3xHxW的格式</li>
<li>original_size：tuple(int, int)格式，存了模型转换前的H和W</li>
<li>point_coords：tensor，图片点的提示，形状为BxNx2</li>
<li>point_labels：tensor，点提示的label，形状为BxN（暂时不清楚）</li>
<li>boxes：tensor，提示框输入，Bx4</li>
<li>mask_input：掩码输入，Bx1xHxW</li>
</ul>
</li>
<li><p>multimask_output</p>
<p>bool，模型是否需要预测混合多个消除掩码歧义，还是返回单个掩码</p>
</li>
</ul>
<h3 id="Return"><a href="#Return" class="headerlink" title="Return"></a>Return</h3><p>类似输入的batched_input</p>
<p>字典包含以下内容：</p>
<ul>
<li>mask：返回的二维掩码，形状是BxCxHxW，C是混合输出的个数（原始图像尺寸大小的mask）</li>
<li>iou_predictions：模型预测的mask质量指标，形状BxC</li>
<li>low_res_logits：低分辨率的logits，形状BxCxHxW，H&#x3D;W&#x3D;256，可以作为掩码输入传递到后续的预测迭代（经过resize后的输入图像）</li>
</ul>
<p>对于每一个输入计算：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">input_images = torch.stack([self.preprocess(x[<span class="string">&quot;image&quot;</span>]) <span class="keyword">for</span> x <span class="keyword">in</span> batched_input], dim=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 将图像输入encoder得到图像嵌入</span></span><br><span class="line">image_embeddings = self.image_encoder(input_images)</span><br><span class="line">outputs = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> image_record, curr_embedding <span class="keyword">in</span> <span class="built_in">zip</span>(batched_input, image_embeddings):</span><br><span class="line">    <span class="keyword">if</span> <span class="string">&quot;point_coords&quot;</span> <span class="keyword">in</span> image_record:</span><br><span class="line">        points = (image_record[<span class="string">&quot;point_coords&quot;</span>], image_record[<span class="string">&quot;point_labels&quot;</span>])</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        points = <span class="literal">None</span></span><br><span class="line">    <span class="comment"># 提示词encoder，得到两个embeding，分别是稀疏和密集型</span></span><br><span class="line">    sparse_embeddings, dense_embeddings = self.prompt_encoder(</span><br><span class="line">        points=points,</span><br><span class="line">        boxes=image_record.get(<span class="string">&quot;boxes&quot;</span>, <span class="literal">None</span>),</span><br><span class="line">        masks=image_record.get(<span class="string">&quot;mask_inputs&quot;</span>, <span class="literal">None</span>),</span><br><span class="line">    )</span><br><span class="line">    <span class="comment"># 用图像decoder计算low res mask和iou</span></span><br><span class="line">    low_res_masks, iou_predictions = self.mask_decoder(</span><br><span class="line">        image_embeddings=curr_embedding.unsqueeze(<span class="number">0</span>),</span><br><span class="line">        image_pe=self.prompt_encoder.get_dense_pe(),</span><br><span class="line">        sparse_prompt_embeddings=sparse_embeddings,</span><br><span class="line">        dense_prompt_embeddings=dense_embeddings,</span><br><span class="line">        multimask_output=multimask_output,</span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 在原图中移除大规模和填充的mask，返回原图尺寸中的mask</span></span><br><span class="line">    masks = self.postprocess_masks(</span><br><span class="line">        low_res_masks,</span><br><span class="line">        input_size=image_record[<span class="string">&quot;image&quot;</span>].shape[-<span class="number">2</span>:],</span><br><span class="line">        original_size=image_record[<span class="string">&quot;original_size&quot;</span>],</span><br><span class="line">    )</span><br><span class="line">    <span class="comment"># mask_threshold = 0</span></span><br><span class="line">    masks = masks &gt; self.mask_threshold</span><br><span class="line">    outputs.append(</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">&quot;masks&quot;</span>: masks,</span><br><span class="line">            <span class="string">&quot;iou_predictions&quot;</span>: iou_predictions,</span><br><span class="line">            <span class="string">&quot;low_res_logits&quot;</span>: low_res_masks,</span><br><span class="line">        &#125;</span><br><span class="line">    )</span><br><span class="line"><span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure>

<h3 id="image-encoder"><a href="#image-encoder" class="headerlink" title="image encoder"></a>image encoder</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> .image_encoder <span class="keyword">import</span> ImageEncoderViT</span><br><span class="line"><span class="comment"># 在集成到SAM之后的引用</span></span><br><span class="line">image_embeddings = self.image_encoder(input_images)</span><br></pre></td></tr></table></figure>

<h3 id="prompt-encoder"><a href="#prompt-encoder" class="headerlink" title="prompt encoder"></a>prompt encoder</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> .prompt_encoder <span class="keyword">import</span> PromptEncoder</span><br><span class="line"><span class="comment"># 在集成到SAM之后的引用</span></span><br><span class="line"><span class="comment"># 提示词encoder，得到两个embeding，分别是稀疏和密集型</span></span><br><span class="line">sparse_embeddings, dense_embeddings = self.prompt_encoder(</span><br><span class="line">    points=points,</span><br><span class="line">    boxes=image_record.get(<span class="string">&quot;boxes&quot;</span>, <span class="literal">None</span>),</span><br><span class="line">    masks=image_record.get(<span class="string">&quot;mask_inputs&quot;</span>, <span class="literal">None</span>),</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>对于点提示</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_embed_points</span>(<span class="params"></span></span><br><span class="line"><span class="params">    self,</span></span><br><span class="line"><span class="params">    points: torch.Tensor,</span></span><br><span class="line"><span class="params">    labels: torch.Tensor,</span></span><br><span class="line"><span class="params">    pad: <span class="built_in">bool</span>,</span></span><br><span class="line"><span class="params"></span>) -&gt; torch.Tensor:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Embeds point prompts.&quot;&quot;&quot;</span></span><br><span class="line">    points = points + <span class="number">0.5</span>  <span class="comment"># Shift to center of pixel</span></span><br><span class="line">    <span class="keyword">if</span> pad:</span><br><span class="line">        padding_point = torch.zeros((points.shape[<span class="number">0</span>], <span class="number">1</span>, <span class="number">2</span>), device=points.device)</span><br><span class="line">        padding_label = -torch.ones((labels.shape[<span class="number">0</span>], <span class="number">1</span>), device=labels.device)</span><br><span class="line">        points = torch.cat([points, padding_point], dim=<span class="number">1</span>)</span><br><span class="line">        labels = torch.cat([labels, padding_label], dim=<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># forward_with_coords是根据点进行位置编码，这里点没有进行归一化，需要按照图像的HW归一化</span></span><br><span class="line">    point_embedding = self.pe_layer.forward_with_coords(points, self.input_image_size)</span><br><span class="line">    point_embedding[labels == -<span class="number">1</span>] = <span class="number">0.0</span></span><br><span class="line">    point_embedding[labels == -<span class="number">1</span>] += self.not_a_point_embed.weight</span><br><span class="line">    point_embedding[labels == <span class="number">0</span>] += self.point_embeddings[<span class="number">0</span>].weight</span><br><span class="line">    point_embedding[labels == <span class="number">1</span>] += self.point_embeddings[<span class="number">1</span>].weight</span><br><span class="line">    <span class="keyword">return</span> point_embedding</span><br></pre></td></tr></table></figure>

<p>对于框提示</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_embed_boxes</span>(<span class="params">self, boxes: torch.Tensor</span>) -&gt; torch.Tensor:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Embeds box prompts.&quot;&quot;&quot;</span></span><br><span class="line">    boxes = boxes + <span class="number">0.5</span>  <span class="comment"># Shift to center of pixel</span></span><br><span class="line">    coords = boxes.reshape(-<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">    <span class="comment"># forward_with_coords是根据点进行位置编码，这里点没有进行归一化，需要按照图像的HW归一化</span></span><br><span class="line">    corner_embedding = self.pe_layer.forward_with_coords(coords, self.input_image_size)</span><br><span class="line">    corner_embedding[:, <span class="number">0</span>, :] += self.point_embeddings[<span class="number">2</span>].weight</span><br><span class="line">    corner_embedding[:, <span class="number">1</span>, :] += self.point_embeddings[<span class="number">3</span>].weight</span><br><span class="line">    <span class="keyword">return</span> corner_embedding</span><br></pre></td></tr></table></figure>

<p>对于mask提示</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_embed_masks</span>(<span class="params">self, masks: torch.Tensor</span>) -&gt; torch.Tensor:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Embeds mask inputs.&quot;&quot;&quot;</span></span><br><span class="line">    mask_embedding = self.mask_downscaling(masks)</span><br><span class="line">    <span class="keyword">return</span> mask_embedding</span><br><span class="line">self.mask_downscaling = nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, mask_in_chans // <span class="number">4</span>, kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">    LayerNorm2d(mask_in_chans // <span class="number">4</span>),</span><br><span class="line">    activation(),</span><br><span class="line">    nn.Conv2d(mask_in_chans // <span class="number">4</span>, mask_in_chans, kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">    LayerNorm2d(mask_in_chans),</span><br><span class="line">    activation(),</span><br><span class="line">    nn.Conv2d(mask_in_chans, embed_dim, kernel_size=<span class="number">1</span>),</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h3 id="mask-decoder"><a href="#mask-decoder" class="headerlink" title="mask decoder"></a>mask decoder</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> .mask_decoder <span class="keyword">import</span> MaskDecoder</span><br><span class="line"><span class="comment"># 在集成到SAM之后的引用</span></span><br><span class="line"><span class="comment"># 输入包含图像和提示的embedding，给出是否需要输出多个mask</span></span><br><span class="line">low_res_masks, iou_predictions = self.mask_decoder(</span><br><span class="line">        image_embeddings=curr_embedding.unsqueeze(<span class="number">0</span>),</span><br><span class="line">    	<span class="comment"># get_dense_pe返回prompt encoder中的适用于</span></span><br><span class="line">    	<span class="comment"># point prompts的指定大小网格的位置编码，形状1x(embed_dim)x(embedding_h)x(embedding_w)</span></span><br><span class="line">        image_pe=self.prompt_encoder.get_dense_pe(),</span><br><span class="line">        sparse_prompt_embeddings=sparse_embeddings,</span><br><span class="line">        dense_prompt_embeddings=dense_embeddings,</span><br><span class="line">        multimask_output=multimask_output,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="自动生成mask"><a href="#自动生成mask" class="headerlink" title="自动生成mask"></a>自动生成mask</h2><p>调用automatic_mask_generator.py中的SamAutomaticMaskGenerator生成实例</p>
<p>传入图像</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">image = cv2.imread(<span class="string">&#x27;images/dog.jpg&#x27;</span>)</span><br><span class="line">image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)</span><br><span class="line"></span><br><span class="line">mask_generator = SamAutomaticMaskGenerator(sam)</span><br><span class="line">masks = mask_generator.generate(image)</span><br></pre></td></tr></table></figure>

<p>masks返回一个mask的列表，每一个mask都包含一个字典，成员包括</p>
<ul>
<li><code>segmentation</code> : the mask</li>
<li><code>area</code> : the area of the mask in pixels</li>
<li><code>bbox</code> : the boundary box of the mask in XYWH format</li>
<li><code>predicted_iou</code> : the model’s own prediction for the quality of the mask</li>
<li><code>point_coords</code> : the sampled input point that generated this mask</li>
<li><code>stability_score</code> : an additional measure of mask quality</li>
<li><code>crop_box</code> : the crop of the image used to generate this mask in XYWH format</li>
</ul>
<p>可以对自动掩码生成器进一步设置</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mask_generator_2 = SamAutomaticMaskGenerator(</span><br><span class="line">    model=sam,</span><br><span class="line">    points_per_side=<span class="number">32</span>,</span><br><span class="line">    pred_iou_thresh=<span class="number">0.86</span>,</span><br><span class="line">    stability_score_thresh=<span class="number">0.92</span>,</span><br><span class="line">    crop_n_layers=<span class="number">1</span>,</span><br><span class="line">    crop_n_points_downscale_factor=<span class="number">2</span>,</span><br><span class="line">    min_mask_region_area=<span class="number">100</span>,  <span class="comment"># Requires open-cv to run post-processing</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>回到automatic_mask_generator.py中查看一下源码</p>
<p>初始化参数：</p>
<ul>
<li><p><strong>model</strong>：（SAM）用于掩码预测的模型</p>
</li>
<li><p><strong>point_per_side</strong>：沿着图像的一侧被采样的点的个数，总点数是point_per_side**2。如果没有，point_grids必须提供明确采样点</p>
<p>（论文中给每一张图固定有多少个采样点）</p>
</li>
<li><p><strong>point_per_batch</strong>：设置同时运行的点数，更高会更快但是需要的gpu内存更高</p>
</li>
<li><p><strong>pred_iou_thresh</strong>：使用模型预测mask的过滤阈值，用于过滤生成的没有到阈值的mask，默认值0.88</p>
</li>
<li><p><strong>stability_score_thresh</strong> ：[0,1]之间的过滤阈值，用于mask变化下的稳定性，默认值0.95</p>
</li>
<li><p>stability_score_offset：计算指标时偏移的截止量</p>
</li>
<li><p>box_nms_thresh ：用于非极大值抑制的cutoff</p>
</li>
<li><p>crop_n_layers：如果&gt;0就会对图像的裁剪再次运行mask预测（对分割结果再进一步分割），设置要运行的层数，每一层有2**i_layer的裁剪数</p>
</li>
<li><p>crop_nms_thresh：非最大抑制使用的框 IoU 截止值，用于过滤重复的mask</p>
</li>
<li><p>crop_overlap_ratio：设置裁剪的重叠程度</p>
</li>
<li><p>crop_n_points_downscale_factor：在第 n 层中每边采样的点数按 crop_n_points_downscale_factor**n 缩小。</p>
</li>
<li><p>point_grids：用于采样点的显示网格列表，归一化到[0,1]，第n个网格用于第n个裁剪层</p>
</li>
<li><p>min_mask_region_area：如果 &gt;0，将应用后处理以移除面积小于 min_mask_region_area 的mask中的断开区域和孔洞</p>
</li>
<li><p>output_mode ：返回的形式掩码。可以是“binary_mask”、“uncompressed_rle”或“coco_rle”。 ‘coco_rle’ 需要 pycocotools。 对于大分辨率，“binary_mask”可能会消耗大量内存。</p>
</li>
</ul>
<p>关于不同参数会对分割结果产生的影响，这里下载了预训练模型在提供的demo上进行了测试</p>
<p>默认参数</p>
<p><img src="C:\Users\Bubble\AppData\Roaming\Typora\typora-user-images\image-20230413164740158.png" alt="image-20230413164740158"></p>
<p>pred_iou_thresh取小值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mask_generator_3 = SamAutomaticMaskGenerator(</span><br><span class="line">    model=sam,</span><br><span class="line">    pred_iou_thresh=<span class="number">0.7</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>





<p>调用generate方法具体执行过程：</p>
<h2 id="根据提示生成mask"><a href="#根据提示生成mask" class="headerlink" title="根据提示生成mask"></a>根据提示生成mask</h2><p>SamPredictor类：</p>
<p>主要方法：</p>
<ul>
<li><p>初始化：输入sam模型</p>
</li>
<li><p>set_image</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">set_image</span>(<span class="params"></span></span><br><span class="line"><span class="params">    self,</span></span><br><span class="line"><span class="params">    image: np.ndarray,</span></span><br><span class="line"><span class="params">    image_format: <span class="built_in">str</span> = <span class="string">&quot;RGB&quot;</span>,</span></span><br><span class="line"><span class="params"></span>) -&gt; <span class="literal">None</span>:</span><br></pre></td></tr></table></figure>

<p>计算所提供图像的图像嵌入，允许使用“predict”方法预测蒙版，这里把图像格式调整到模型输入要求，传给并调用set_torch_image方法</p>
<p>参数：</p>
<ul>
<li>image：要计算mask的图像的ndarray数组，希望是HWC和uint8格式，像素值[0,255]</li>
<li>image_format：色彩通道格式，RGB或者BGR</li>
</ul>
</li>
<li><p>set_torch_image</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">set_torch_image</span>(<span class="params"></span></span><br><span class="line"><span class="params">    self,</span></span><br><span class="line"><span class="params">    transformed_image: torch.Tensor,</span></span><br><span class="line"><span class="params">    original_image_size: <span class="type">Tuple</span>[<span class="built_in">int</span>, ...],</span></span><br><span class="line"><span class="params"></span>) -&gt; <span class="literal">None</span>:</span><br></pre></td></tr></table></figure>

<p>对已经转化到输入tensor格式的图像，计算图像嵌入，允许使用“predict”方法预测蒙版</p>
<p>参数</p>
<ul>
<li>transformed_image：转化好格式的图像</li>
<li>图像在转化之前的尺寸大小，(H,W)形式</li>
</ul>
<p>这个方法把image的tensor输入image encoder中得到特征</p>
</li>
<li><p>predict</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params"></span></span><br><span class="line"><span class="params">    self,</span></span><br><span class="line"><span class="params">    point_coords: <span class="type">Optional</span>[np.ndarray] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    point_labels: <span class="type">Optional</span>[np.ndarray] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    box: <span class="type">Optional</span>[np.ndarray] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    mask_input: <span class="type">Optional</span>[np.ndarray] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    multimask_output: <span class="built_in">bool</span> = <span class="literal">True</span>,</span></span><br><span class="line"><span class="params">    return_logits: <span class="built_in">bool</span> = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params"></span>) -&gt; <span class="type">Tuple</span>[torch.Tensor, torch.Tensor, torch.Tensor]:</span><br></pre></td></tr></table></figure>

<p>使用给的输入prompt预测masks，实际上调用了predict_torch方法</p>
<p>参数：</p>
<ul>
<li>point_coords:np数组或者none，一个N*2的数组，每个数组是图中像素(x,y)坐标</li>
<li>point_labels:长度为N的数组，每个点prompt的label，1表示前景，0表示背景</li>
<li>box:长度为4的数组，给框prompt，XYXY格式</li>
<li>mask_input:给模型的低resolution的mask输入，通常来自前面的预测迭代，1xHxW的格式，在sam中，H&#x3D;W&#x3D;256</li>
<li>multimask_output:如果是true，模型返回三个mask。对于不明确的输入prompt，通常会产生比单个预测更好的mask。如果只需要一个mask，可以使用模型的打分来选择最佳mask，对于没有歧义的prompt，取false可以给出更好的结果</li>
<li>return_logits:如果true，返回非阈值mask logits而不是二进制mask（不太理解）</li>
</ul>
<p>return：</p>
<ul>
<li>CxHxW格式的输出mask，C是mask数量，HW是原始图像大小</li>
<li>一个长度为C的数组，包含模型对每个mask质量的打分</li>
<li>形状是CxHxW的数组，C是mask数量，WH是256，这些可以作为mask输入传递给后续迭代</li>
</ul>
</li>
<li><p>predict_torch</p>
<p>根据输入的prompt预测mask，输入的prompt都是torch.tensor，已经使用ResizeLongestSide转换过</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">predict_torch</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        point_coords: <span class="type">Optional</span>[torch.Tensor],</span></span><br><span class="line"><span class="params">        point_labels: <span class="type">Optional</span>[torch.Tensor],</span></span><br><span class="line"><span class="params">        boxes: <span class="type">Optional</span>[torch.Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        mask_input: <span class="type">Optional</span>[torch.Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        multimask_output: <span class="built_in">bool</span> = <span class="literal">True</span>,</span></span><br><span class="line"><span class="params">        return_logits: <span class="built_in">bool</span> = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">    </span>) -&gt; <span class="type">Tuple</span>[torch.Tensor, torch.Tensor, torch.Tensor]:</span><br></pre></td></tr></table></figure></li>
</ul>
<h1 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h1><p>任何应用都可以按照：任务，模型，数据三个维度来考虑</p>
<p>数据引擎的做法在论文和项目中很少涉及到，只是提供了最后的数据集</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/06/15/Segment%20Anything/" data-id="clix3qjvn000aqwueax2m41xq" data-title="" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-SCALE EFFICIENTLY INSIGHTS FROM PRE-TRAINING AND FINE-TUNING TRANSFORMERS" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/06/15/SCALE%20EFFICIENTLY%20INSIGHTS%20FROM%20PRE-TRAINING%20AND%20FINE-TUNING%20TRANSFORMERS/" class="article-date">
  <time class="dt-published" datetime="2023-06-15T12:12:08.526Z" itemprop="datePublished">2023-06-15</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="高效扩展：来自预训练和微调Transformer的见解（Google-2022）"><a href="#高效扩展：来自预训练和微调Transformer的见解（Google-2022）" class="headerlink" title="高效扩展：来自预训练和微调Transformer的见解（Google 2022）"></a>高效扩展：来自预训练和微调Transformer的见解（Google 2022）</h1><p>主要的观点与发现：</p>
<ol>
<li>除了模型大小，模型的结构对下游微调很重要</li>
<li>缩放协议在不同计算区域运行不同</li>
<li>广泛采用的 T5-base 和 T5-large 尺寸是pareto低效的。</li>
</ol>
<p>提出了DeepNarrow策略</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/06/15/SCALE%20EFFICIENTLY%20INSIGHTS%20FROM%20PRE-TRAINING%20AND%20FINE-TUNING%20TRANSFORMERS/" data-id="clix3qjvm0008qwueagrqacea" data-title="" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-sam组会报告" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/06/15/sam%E7%BB%84%E4%BC%9A%E6%8A%A5%E5%91%8A/" class="article-date">
  <time class="dt-published" datetime="2023-06-15T12:12:08.518Z" itemprop="datePublished">2023-06-15</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="SAM介绍"><a href="#SAM介绍" class="headerlink" title="SAM介绍"></a>SAM介绍</h1><p><img src="C:\Users\Bubble\AppData\Roaming\Typora\typora-user-images\image-20230410155322999.png" alt="image-20230410155322999"></p>
<p>simpleClick</p>
<p><img src="C:\Users\Bubble\AppData\Roaming\Typora\typora-user-images\image-20230508135906842.png" alt="image-20230508135906842"></p>
<h1 id="SAM使用"><a href="#SAM使用" class="headerlink" title="SAM使用"></a>SAM使用</h1><h2 id="SAM怎么结合语义分割的架构微调"><a href="#SAM怎么结合语义分割的架构微调" class="headerlink" title="SAM怎么结合语义分割的架构微调"></a>SAM怎么结合语义分割的架构微调</h2><h1 id="SAM在医学领域的性能"><a href="#SAM在医学领域的性能" class="headerlink" title="SAM在医学领域的性能"></a>SAM在医学领域的性能</h1><p>​		SAM基于单一提示的性能差异很大，即从脊柱MRI数据集的0.1135到髋关节x射线数据集的0.8650，由IoU评估。对于包括具有明确提示的边界清晰的对象在内的任务，性能似乎较高，而在许多其他场景（如肿瘤分割）中，性能较差。当提供多个提示时，总体性能仅略有提高，但对于对象不连续的数据集，性能更为提高。</p>
<p><img src="C:\Users\Bubble\AppData\Roaming\Typora\typora-user-images\image-20230508144451624.png" alt="image-20230508144451624"></p>
<p><img src="C:\Users\Bubble\AppData\Roaming\Typora\typora-user-images\image-20230508144507488.png" alt="image-20230508144507488"></p>
<p><img src="C:\Users\Bubble\AppData\Roaming\Typora\typora-user-images\image-20230508144535151.png" alt="image-20230508144535151"></p>
<p><img src="C:\Users\Bubble\AppData\Roaming\Typora\typora-user-images\image-20230508144613354.png" alt="image-20230508144613354"></p>
<h1 id="SAM在医学领域的尝试"><a href="#SAM在医学领域的尝试" class="headerlink" title="SAM在医学领域的尝试"></a>SAM在医学领域的尝试</h1><ol>
<li><p>结合到各种软件中方便使用</p>
</li>
<li><p><strong>Input Augmentation with SAM: Boosting Medical Image Segmentation with Segmentation Foundation Model</strong></p>
<p><img src="C:\Users\Bubble\AppData\Roaming\Typora\typora-user-images\image-20230509143557429.png" alt="image-20230509143557429"></p>
<p>使用sam来生成特定任务的扩充数据</p>
<p>分割增强：创建一个3通道图像，其中第一通道由灰度级原始图像组成，第二通道由其分割先验图组成，第三通道由其边界先验图组成。</p>
</li>
<li><p>适配器</p>
<p>适配器的概念是在NLP中提出的，给可拓展大型模型为下游任务微调</p>
<ul>
<li><p>Medical SAM Adapter: Adapting Segment Anything Model for Medical Image Segmentation</p>
<p>通过使用Adapter按照参数有效的微调范式对预训练的SAM模型进行微调。</p>
<p><img src="C:\Users\Bubble\AppData\Roaming\Typora\typora-user-images\image-20230509150235520.png" alt="image-20230509150235520"></p>
</li>
<li><p>SAM Fails to Segment Anything? – SAM-Adapter: Adapting SAM in Underperformed Scenes: Camouflage, Shadow, Medical Image Segmentation, and More</p>
<p><img src="C:\Users\Bubble\AppData\Roaming\Typora\typora-user-images\image-20230509151954661.png" alt="image-20230509151954661"></p>
<p><img src="C:\Users\Bubble\AppData\Roaming\Typora\typora-user-images\image-20230509152211718.png" alt="image-20230509152211718"></p>
</li>
</ul>
</li>
<li><p>Segment Anything in Medical Images</p>
<p><img src="C:\Users\Bubble\AppData\Roaming\Typora\typora-user-images\image-20230509152634667.png" alt="image-20230509152634667"></p>
</li>
<li><p>Customized Segment Anything Model for Medical Image Segmentation</p>
</li>
</ol>
<p>SAMed将基于低秩（LoRA）的微调策略应用于SAM图像编码器（冻结图像编码器，并采用基于低秩的微调策略（LoRA）来近似图像编码器中参数的低秩更新），并在标记的医学图像分割数据集上与提示编码器和掩码解码器一起对其进行微调。微调SAM的轻量级提示编码器和掩码解码器</p>
<p>预热和AdamW优化器可以极大地稳定微调过程，从而提高分割精度</p>
<p><img src="C:\Users\Bubble\AppData\Roaming\Typora\typora-user-images\image-20230516145145426.png" alt="image-20230516145145426"></p>
<p><img src="C:\Users\Bubble\AppData\Roaming\Typora\typora-user-images\image-20230516145140488.png" alt="image-20230516145140488"></p>
<p><img src="C:\Users\Bubble\AppData\Roaming\Typora\typora-user-images\image-20230516145549363.png" alt="image-20230516145549363"></p>
<h1 id="SAM-amp-amp-Medical"><a href="#SAM-amp-amp-Medical" class="headerlink" title="SAM &amp;&amp; Medical"></a>SAM &amp;&amp; Medical</h1><ul>
<li>综述，能力评估</li>
<li>数据扩充</li>
<li>微调</li>
</ul>
<p>问题：</p>
<ol>
<li>在特定任务上，SAM与专用的分割模型的性能？</li>
<li>预训练权重</li>
</ol>
<p>异位萌出分类：</p>
<p>1类？2类？全牙类？</p>
<p>分割每一颗牙齿</p>
<p>分割异常牙齿</p>
<p>方向1：利用SAM和nnUNet等工具</p>
<p>方向2：研究大模型的微调（利用NLP，AIGC等领域的研究）</p>
<p>我们需要什么：</p>
<ul>
<li>sam在大量数据训练下的权重</li>
<li></li>
</ul>
<p>如果sam要到我们的任务上微调，微调什么？</p>
<h2 id="专业的人做专业的事——使用SAM通用大模型辅助专用模型训练"><a href="#专业的人做专业的事——使用SAM通用大模型辅助专用模型训练" class="headerlink" title="专业的人做专业的事——使用SAM通用大模型辅助专用模型训练"></a>专业的人做专业的事——使用SAM通用大模型辅助专用模型训练</h2><p>出发：在特定领域，即使微调SAM，也很难超过专门模型的精度。对于未知数据，先利用大型模型SAM来对数据初步分割，后使用专家模型进一步精准分割</p>
<ol>
<li>标记特定任务少量数据label（10个左右）</li>
<li>使用nnUNet训练特定任务的第一阶段模型（专家模型）</li>
<li><strong>使用第一阶段模型预测所有数据的label（最好区域小于gt在gt内部）</strong></li>
<li><strong>将预测得到的label作为mask的prompt与原图输入SAM</strong></li>
<li>根据SAM的分割结果再次训练特定任务模型</li>
</ol>
<p>问题：</p>
<ul>
<li>mask预测的不可控</li>
<li>sam的mask输入，效果不佳，github上很多人反馈，做了很多处理都达不到其他prompt的效果，目前没有很好的解决方案#169，#242，#360</li>
<li>将mask采样成点？框？目标检测？做一个mask适配器，将输入mask转换成高效的prompt</li>
</ul>
<h2 id="prompt新范式"><a href="#prompt新范式" class="headerlink" title="prompt新范式"></a>prompt新范式</h2><p>问题：SAM在没有修改之前，可以使用足够的promot来精确分割牙齿吗</p>
<h2 id="研究大模型的微调（利用NLP，AIGC等领域的研究）"><a href="#研究大模型的微调（利用NLP，AIGC等领域的研究）" class="headerlink" title="研究大模型的微调（利用NLP，AIGC等领域的研究）"></a>研究大模型的微调（利用NLP，AIGC等领域的研究）</h2>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/06/15/sam%E7%BB%84%E4%BC%9A%E6%8A%A5%E5%91%8A/" data-id="clix3qjvr000hqwueg3y3b8y7" data-title="" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  


  <nav id="page-nav">
    
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/">Next &raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/06/">June 2023</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2023/06/15/sam%20related%20note/">(no title)</a>
          </li>
        
          <li>
            <a href="/2023/06/15/Sam%20on%20tooth/">(no title)</a>
          </li>
        
          <li>
            <a href="/2023/06/15/nnUNetv2%E7%AC%94%E8%AE%B0/">(no title)</a>
          </li>
        
          <li>
            <a href="/2023/06/15/MedNeXt%20Transformer-driven%20Scaling%20of%20ConvNets%20for%20Medical%20Image%20Segmentation/">(no title)</a>
          </li>
        
          <li>
            <a href="/2023/06/15/%E5%BC%82%E4%BD%8D%E8%90%8C%E5%87%BA%E4%BB%BB%E5%8A%A1/">(no title)</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2023 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>